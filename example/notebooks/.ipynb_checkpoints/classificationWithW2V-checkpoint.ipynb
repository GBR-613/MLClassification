{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Politics 9703\n",
      "2 Sport 9866\n",
      "3 Culture 8076\n",
      "4 Economy 10090\n",
      "Load input data in 12 sec\n",
      "37735 docs: 25658 train, 4528 validation, 7549 test\n",
      "Documents length: maximum: 6170, minimum: 5\n",
      "Continue\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "#from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from collections import OrderedDict\n",
    "import pickle as pkl\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "from keras.models import load_model\n",
    "import random\n",
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "homePath = str(Path.home()) + \"/MLClassification/example/data/\"\n",
    "LabeledSentence = gensim.models.doc2vec.TaggedDocument\n",
    "n_dim = 100\n",
    "n_cats = 4\n",
    "balance = 0\n",
    "modelType = 2\n",
    "wvModelType = 'c-fasttext'\n",
    "batch_size = 128\n",
    "numpy.random.seed = 7\n",
    "dataset_path= homePath + 'docs'\n",
    "best_models_path = homePath + 'bestModels/'\n",
    "wordVecType = 0\n",
    "trainPart = 0.8\n",
    "testPart = 0.2\n",
    "valPart = 0.15\n",
    "cnt = 0\n",
    "fCats = [0] * n_cats\n",
    "nCats = []\n",
    "\n",
    "LabeledDocument = namedtuple('LabeledDocument', 'words tags docClass docCategory')\n",
    "\n",
    "def prepareDocsData(path):\n",
    "    global cnt\n",
    "    global lang\n",
    "    wolabPath = homePath + 'wolab.txt'\n",
    "    tmpf = open(wolabPath, 'w', encoding='UTF-8')\n",
    "    docs = []\n",
    "    trainCount = 0\n",
    "    testCount = 0\n",
    "    wCount = 0\n",
    "    fCount = 0\n",
    "    wordInDocs = dict()\n",
    "    currdir = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    for ff in glob.glob(\"*\"):\n",
    "        if ff[-3:] == 'txt':\n",
    "            continue\n",
    "        #print (ff)\n",
    "        cnt = cnt + 1;\n",
    "        tf = path + \"/\" + ff + \"/tocs.txt\"\n",
    "        with open(tf, 'r', encoding='UTF-8') as f:\n",
    "            docsCount = 0 \n",
    "            for line in f:\n",
    "                if line[:1] == '#':\n",
    "                    if (balance > 0 and docsCount >= balance):\n",
    "                        break;\n",
    "                    docsCount = docsCount + 1\n",
    "        f.close()\n",
    "        fCats[cnt - 1] = docsCount\n",
    "        nCats.append(ff)\n",
    "        print (cnt, ff, docsCount)\n",
    "        with open(tf, 'r', encoding='UTF-8') as f:\n",
    "            dCount = 0\n",
    "            wordInDoc = dict()\n",
    "            for line in f:\n",
    "                if line[:1] == '#':\n",
    "                    if (dCount <= docsCount):\n",
    "                        dCount = dCount + 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break;\n",
    "                if dCount <= int(docsCount * trainPart):\n",
    "                    group = 'train'\n",
    "                    trainCount = trainCount + 1\n",
    "                    fCount = trainCount\n",
    "                elif dCount <= int(docsCount * (trainPart + testPart)):\n",
    "                    group = 'tests'\n",
    "                    testCount = testCount + 1\n",
    "                    fCount = testCount\n",
    "                else:\n",
    "                    group = 'wolab'\n",
    "                    wCount = wCount + 1\n",
    "                    fCount = wCount\n",
    "                    tmpf.write(str(cnt-1) + \"\\t\" + line.strip() + \"\\n\")\n",
    "                tag = '%s_%d'%(group, fCount)\n",
    "                if group != 'wolab':\n",
    "                    words = line.strip().split()\n",
    "                    if len(words) < 5:\n",
    "                        print(\"Attention! Doc %d from category %s has only %d tokens.\"%(dCount, ff, len(words)))\n",
    "                    docs.append(LabeledDocument(words, [tag], ff, cnt-1))\n",
    "                    if group == 'train':\n",
    "                        for w in words:\n",
    "                            if w not in wordInDoc:\n",
    "                                wordInDoc[w] = 1\n",
    "                                if w not in wordInDocs:\n",
    "                                    wordInDocs[w] = 1\n",
    "                                else:\n",
    "                                    wordInDocs[w] = wordInDocs[w] + 1\n",
    "\n",
    "        f.close()\n",
    "    tmpf.close()\n",
    "    return docs, wordInDocs\n",
    "\n",
    "ds = datetime.datetime.now()\n",
    "allDocs, wordInDocs = prepareDocsData(dataset_path)\n",
    "maxDocLen = max(len(x.words) for x in allDocs)\n",
    "minDocLen = min(len(x.words) for x in allDocs)\n",
    "trainAllDocs = [doc for doc in allDocs if doc.tags[0][:5] == 'train']\n",
    "trainAllDocs = random.sample(trainAllDocs, len(trainAllDocs))\n",
    "trainDocs = trainAllDocs[:int(len(trainAllDocs) * (1 - valPart))]\n",
    "valDocs = trainAllDocs[int(len(trainAllDocs) * (1 - valPart)):]\n",
    "\n",
    "testDocs = [doc for doc in allDocs if doc.tags[0][:5] == 'tests']\n",
    "testDocs = random.sample(testDocs, len(testDocs))\n",
    "de = datetime.datetime.now()\n",
    "\n",
    "print (\"Load input data in %d sec\"%((de-ds).total_seconds())) \n",
    "print('%d docs: %d train, %d validation, %d test' % (len(allDocs), len(trainDocs), len(valDocs), len(testDocs)))\n",
    "print(\"Documents length: maximum: %d, minimum: %d\"%(maxDocLen, minDocLen))\n",
    "del allDocs\n",
    "del trainAllDocs\n",
    "print (\"Continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load W2V model (c-fasttext) in 55 sec\n",
      "Continue\n"
     ]
    }
   ],
   "source": [
    "ds = datetime.datetime.now()\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(homePath + 'wiki_ar.vec')\n",
    "de = datetime.datetime.now()\n",
    "print (\"Load W2V model (%s) in %s sec\"%(wvModelType, (de-ds).seconds)) \n",
    "print (\"Continue\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "import math\n",
    "\n",
    "tmpCount = 0\n",
    "\n",
    "def getWordVector(tokens, modelType, wordInDocs, wvModel):\n",
    "    global n_dim\n",
    "    global tmpCount\n",
    "    global wordVecType\n",
    "    sdict = dict()\n",
    "    tmpCount = tmpCount + 1\n",
    "    if tmpCount != 0 and tmpCount%1000 == 0:\n",
    "        print(\"Load \", tmpCount, end=\"\\r\")\n",
    "        \n",
    "    if wordVecType == 2:\n",
    "        firstStep = True;\n",
    "        vec = numpy.zeros(n_dim)\n",
    "        v2 = numpy.zeros(n_dim)\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                if firstStep == True:\n",
    "                    vec = wvModel[word]\n",
    "                    v2 = wvModel[word]\n",
    "                    firstStep = False\n",
    "                else:\n",
    "                    vec = numpy.minimum(vec, wvModel[word])\n",
    "                    v2 = numpy.maximum(v2, wvModel[word])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        dem = len(vec)\n",
    "        vec = numpy.concatenate((vec,v2)).reshape(1, dem*2)\n",
    "        return vec\n",
    "        \n",
    "    vec = numpy.zeros(n_dim).reshape((1, n_dim))\n",
    "    count = 0.\n",
    "    if wordVecType == 1:\n",
    "        for word in tokens:\n",
    "            if word not in sdict:\n",
    "                sdict[word] = 1\n",
    "            else:\n",
    "                sdict[word] = sdict[word] + 1\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += wvModel[word].reshape((1, n_dim)) \n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if wordVecType < 2 and count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# create the model\n",
    "def getModel(modelType, n_dim):\n",
    "    global batch_size\n",
    "    print(\"Model type: %d\"%(modelType))\n",
    "    model = Sequential()\n",
    "    if modelType == 0:\n",
    "        print (\"Dence model\")\n",
    "        model.add(Dense(256, activation='relu', input_dim=n_dim))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(n_cats, activation='softmax'))\n",
    "    elif modelType == 1:\n",
    "        print(\"RNN model\")\n",
    "        model.add(Reshape((1, n_dim), input_shape=(n_dim,)))\n",
    "        model.add(LSTM(200,  input_shape=(1, n_dim),  return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(n_cats, activation='softmax'))\n",
    "    elif modelType == 2:\n",
    "        print(\"CNN model\")\n",
    "        model.add(Reshape((1, n_dim), input_shape=(n_dim,)))\n",
    "        model.add(Conv1D(input_shape=(1, n_dim),\n",
    "                        filters=100,\n",
    "                        kernel_size=1,\n",
    "                        padding=\"valid\",\n",
    "                        activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(1))\n",
    "        model.add(Flatten())        \n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(n_cats, activation='softmax'))\n",
    "        \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model    \n",
    "\n",
    "print (\"Continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  2000\r"
     ]
    }
   ],
   "source": [
    "ds = datetime.datetime.now()\n",
    "train_arrays = numpy.concatenate([getWordVector(x.words, modelType, wordInDocs, w2v) for x in trainDocs])\n",
    "val_arrays = numpy.concatenate([getWordVector(x.words, modelType, wordInDocs, w2v) for x in valDocs])\n",
    "train_labels = numpy.array([y.docCategory for y in trainDocs])\n",
    "val_labels = numpy.array([y.docCategory for y in valDocs])\n",
    "\n",
    "#print (len(train_arrays), train_arrays.shape)\n",
    "tmpCount = 0\n",
    "\n",
    "de = datetime.datetime.now()\n",
    "print (\"Prepare train and validation data in %d sec\"%((de-ds).total_seconds()))\n",
    "ds = datetime.datetime.now()\n",
    "\n",
    "test_arrays = numpy.concatenate([getWordVector(x.words, modelType, wordInDocs, w2v) for x in testDocs])\n",
    "test_labels = numpy.array([y.docCategory for y in testDocs])\n",
    "\n",
    "de = datetime.datetime.now()\n",
    "print (\"Prepare test data in %d sec\"%((de-ds).total_seconds()))\n",
    "\n",
    "print (\"train data size: \", len(train_arrays))\n",
    "print (\"test data size: \", len(test_arrays))\n",
    "print (\"validation data size: \", len(val_arrays))\n",
    "\n",
    "print (\"Continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dms = n_dim\n",
    "if wordVecType == 2:\n",
    "    dms *= 2\n",
    "model = getModel(modelType, dms)\n",
    "eps = [20, 20, 20]\n",
    "checkpoint = ModelCheckpoint(best_models_path + 'curModel%d.hdf5'%(modelType), monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "#print (model.summary())\n",
    "#print (model.metrics_names)\n",
    "\n",
    "model.fit(train_arrays, train_labels, epochs=eps[modelType], validation_data=(val_arrays, val_labels), batch_size=batch_size, verbose=1, callbacks=[checkpoint], shuffle=False)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_arrays, test_labels, verbose=1)\n",
    "print(\"Final model accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "model = load_model(best_models_path + 'curModel%d.hdf5'%(modelType))\n",
    "scores = model.evaluate(test_arrays, test_labels, verbose=1)\n",
    "print(\"Best model accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "correct = 0\n",
    "arrCats = [0] * n_cats\n",
    "chCats = [0] * n_cats\n",
    "zCats = [[0 for x in range(n_cats)] for y in range(n_cats)] \n",
    "tpCats = [[[0 for z in range(4)] for y in range(n_cats)] for x in range(3)]\n",
    "#print(tpCats.shape)\n",
    "\n",
    "for i in range(len(test_arrays)):\n",
    "    count = i + 1\n",
    "    vec = test_arrays[i].reshape(1,100)\n",
    "    category = test_labels[i]\n",
    "    a = model.predict_classes((vec))\n",
    "    chCats[category] = chCats[category] + 1\n",
    "    if int(a[0]) == category:\n",
    "        correct = correct +1\n",
    "    else:\n",
    "        arrCats[category] = arrCats[category] + 1\n",
    "        zCats[category][int(a[0])] = zCats[category][int(a[0])] + 1 \n",
    "            \n",
    "print(\"DL Accuracy: %.2f%%\" % (correct/count*100))\n",
    "print(\"Results of prediction docs by category:\")\n",
    "print(\"\\n%2s %15s %9s %8s %6s  %7s  %8s    %5s    %5s    %3s\"%(\"N\",\"Category\",\"At all\",\"In test\",\"TP\",\"FN\",\"FP\",\"Recall\",\"Precis.\",\"F1\"))\n",
    "print(\"=============================================================================================\")\n",
    "for i in range(n_cats):\n",
    "    fp = 0\n",
    "    for j in range(n_cats):\n",
    "        if j == i:\n",
    "            continue\n",
    "        fp = fp + zCats[j][i]\n",
    "    recall = (chCats[i] - arrCats[i])/chCats[i]*100\n",
    "    precision = (chCats[i] - arrCats[i])/(chCats[i] - arrCats[i] + fp)*100\n",
    "    f1 = 2*(recall * precision) / (recall + precision)\n",
    "    print(\"%2d %15s %8d %8d %8d %8d %8d    %.2f%%    %.2f%%    %.2f%%\"%(i+1, nCats[i], fCats[i], chCats[i], \n",
    "          chCats[i] - arrCats[i], arrCats[i], fp, recall, precision, f1)) \n",
    " \n",
    "print (\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
