[root]
# Root folder of the data. Empty value means user's home.
home =
# Show plots
show_plots = no

[preprocess]
# Defines the need in language-specific tokenization.
language_tokenization = yes
# Relative path to the folder, containing source data. Example: data/bbcnews/source.
source_path = MLClassificationData/bbcnews
# Relative path to the folder, containing results of tokenization. Example: data/bbcnews/target.
target_path = MLClassificationData/bbcnews3
# Relative path to the tagger's jar
set_of_docs_lang_tokenization_lib_path = MLClassification/tokenizer/arabic/stanford/taggers/PyArabicTokenizer.jar
# Relative path to runtime version of tagger. The same jar used by DataLoader.
single_doc_lang_tokenization_lib_path = MLClassification/tokenizer/arabic/stanford/taggers/ArabicDocumentsTokenizer.jar
# List of POS's, which should be excluded from results of tokenization.
exclude_positions = PUNC,DT,IN,CD,PRP,RP,RB,W,PDT
# Need in text normalization.
normalization = yes
# Need to exclude stop words from results of tokenization.
stop_words = yes
# List of extra words, which should be excluded from results of tokenization.
extra_words =

[word_embedding]
# Need to recreate W2V model.
need_create_model = yes
# Relative path to the text corpus.
data_corpus_path = MLClassificationData/w2v/target/wiki_ar.txt
# Dimentions of vectors.
vectors_dimension = 100
# Count epochs in training
epochs_total = 100
# Add time stamp to the model's name
include_current_time_in_model_name = no
# Relative path to W2V vectors file
model_path = MLClassificationData/w2v/vectors/W2VModel.vec

[data]
# Need in language-specific tokenization
language_tokenization = yes
# Relative path to the folder, containing train or all data.
#train_data_path = MLClassificationData/train/rtanews/target
train_data_path = MLClassificationData/train/rtanews/target
# Relative path to the folder, containing test data.
test_data_path = MLClassificationData/test/rtanews/target
# Size of test data set as a part of train data set (used if test_data_path is empty).
test_data_size = 0
# Size of validation data set as a part of train data set.
validation_data_size = 0.15
# List of categories, excluded from training and testing.
exclude_categories =
# Need to show data set analysis.
analysis = no
# Need to load w2v model
load_w2v_model = yes
# Tokenization of loaded data
enable_tokenization = no
# Path to the folder, containing actual documents for testing
actualPath = MLClassificationData/test/rtanews/source

[model]
#  Type of the model. One of SNN, LTSM, CNN, PAC, Perceptron, Ridge, SGD, SVC, BERT
type =
#  Name of the model.
name =
# Default count of epochs in training.
epochs = 20
# Batch size for training.
train_batch = 128
# Batch size for testing.
test_batch = 8
# Training and testing verbose.
verbose = 1
# Need to save intermediate results.
save_intermediate_results = yes
# Relative path to the folder with intermediate results.
intermediate_results_path = MLClassificationData/models/temp
# Relative path to created model.
created_model_path = MLClassificationData/models
# Path to indexer
indexer_path = MLClassificationData/indexers/indexer.pkl
# Path to binarizer
binarizer_path = MLClassificationData/indexers/mlb.pkl
# Path to vectorizer
vectorizer_path = MLClassificationData/indexers/wev.pkl
# Pre-trained BERT model path
pretrained_bert_model_path = MLClassificationData/pybert/pytorch_bert.gz
# Path to folder with resulting BERT files
resulting_bert_files_path = MLClassificationData/pybert/out
# Type of execution. One of trainAndTest, train, test, crossValidation and none
type_of_execution = trainandtest
# Count of cross-validation's loops.
cross_validations_total = 10
# Need to save datasets, correpond to cross-val. cycle with the best results
save_cross_validations_datasets = yes
# Path to the folder containing train and test datasets used in cross-val. loop with the best results
cross_validations_datasets_path = MLClassificationData/crossValidation
# Show results of testing
show_test_results = yes

[collector]
# Show consolidated results
show_consolidated_results = no
# Calculate and save reports
reports = yes
# Path to the folder containing reportsccccccccccccccc
reports_path = MLClassificationData/reports
# Prepare resources for runtime
prepare_resources_for_runtime = yes
# Path to the folder containing saved resources
saved_resources_path = MLClassificationData/runtime

# === Requests ===
# Request defines the pipe - chain of processes, in which previous processes can prepare data or set parameters
# for subsequent. Processes are separated by symbol '|'. Currently we support 5 types of processes:
# W - word embedding
# P - preprocess
# D - data loading
# M - create/train/test model
# C - collector (consolidate results of model's testing and save resources for runtime)
# Each process has the following structure:
# <Symbol_of_process>(<list_of_parameters)
# List of parameters is a list of configuration's options, which should be changed for current and subsequent
# processes.
[requests]
request = D(load_w2v_model=no) | M(type=svc; name=svc; type_of_execution=crossvalidation)
#request = P()
#request = D(enable_tokenization=yes) | M(type=snn; name=snn; epochs=30) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(load_w2v_model=no) | M(type=cnn; name=cnn; epochs=10) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(load_w2v_model=no) | M(type=perceptron; name=perceptron; type_of_execution=crossvalidation)
#request = D(load_w2v_model=no; train_data_path=MLClassificationData/crossValidation/train; test_data_path=MLClassificationData/crossValidation/test) | M(type=perceptron; name=perceptron)
infofrom = 2 days
# days(s), today
