[root]
# Root folder of the data. Empty value means user's home.
home = /home/user/PROJECTS
# Show plots
show_plots = False

[preprocess]
# Set False for pure Python version of Stanford NLP
use_java = True
# Defines the need in language-specific tokenization.
language_tokenization = True
# Relative path to the folder, containing source data. Example: data/bbcnews/source.
source_path = MLClassificationData/bbcnews
# Relative path to the folder, containing results of tokenization. Example: data/bbcnews/target.
target_path = MLClassificationData/bbcnews3
# Relative path to the tagger's jar (wrapper over Stanford Java NLP library, including stopwords etc.)
set_of_docs_lang_tokenization_lib_path = MLClassification/tokenizer/arabic/stanford/taggers/PyArabicTokenizer.jar
# Relative path to runtime version of tagger. The same jar used by DataLoader.
# It is wrapper over Stanford Java NLP library only. Preprocessing including stopwords etc. is done in Python code
single_doc_lang_tokenization_lib_path = MLClassification/tokenizer/arabic/stanford/taggers/ArabicDocumentsTokenizer.jar
# List of POS's, which should be excluded from results of tokenization.
exclude_positions = PUNC,DT,IN,CD,PRP,RP,RB,W,PDT
# Need in text normalization.
normalization = True
# Need to exclude stop words from results of tokenization.
stop_words = True
# List of extra words, which should be excluded from results of tokenization.
extra_words =

[word_embedding]
# Need to recreate W2V model.
need_create_model = True
# Relative path to the text corpus.
data_corpus_path = MLClassificationData/w2v/target/wiki_ar.txt
# Dimentions of vectors.
vectors_dimension = 100
# Count epochs in training
epochs_total = 100
# Add time stamp to the model's name
include_current_time_in_model_name = False
# Relative path to W2V vectors file
#model_path = MLClassificationData/w2v/vectors/W2VModel.vec
model_path = MLClassificationData/model-2019-Feb-25-174112.vec

[data]
# Set False for pure Python version of Stanford NLP
use_java = True
# Need in language-specific tokenization
#language_tokenization = True
# Relative path to the folder, containing train or all data.
#train_data_path = MLClassificationData/train/rtanews/target
train_data_path = MLClassificationData/train/rtanews/target
# Relative path to the folder, containing test data.
test_data_path = MLClassificationData/test/rtanews/target
# Size of test data set as a part of train data set (used if test_data_path is empty).
test_data_size = 0
# Size of validation data set as a part of train data set.
validation_data_size = 0.15
# List of categories, excluded from training and testing.
exclude_categories =
# Need to show data set analysis.
analysis = False
# Need to load w2v model
load_w2v_model = True
# Tokenization of loaded data
enable_tokenization = False
# Path to the folder, containing actual documents for testing
actual_path = MLClassificationData/test/rtanews/source

[model]
#  Type of the model. One of SNN, LTSM, CNN, PAC, Perceptron, Ridge, SGD, SVC, BERT
type =
#  Name of the model.
name =
# Default count of epochs in training.
epochs = 20
# Batch size for training.
train_batch = 128
# Batch size for testing (not implemented?).
test_batch = 8
# Training and testing verbose.
verbose = 1
# Need to save intermediate results.
save_intermediate_results = True
# Relative path to the folder with intermediate results.
intermediate_results_path = MLClassificationData/models/temp
# Relative path to created model.
created_model_path = MLClassificationData/models
# Path to indexer
indexer_path = MLClassificationData/indexers/indexer.pkl
# Path to binarizer
binarizer_path = MLClassificationData/indexers/mlb.pkl
# Path to vectorizer
vectorizer_path = MLClassificationData/indexers/wev.pkl
# Pre-trained BERT model path
pretrained_bert_model_path = MLClassificationData/pybert/pytorch_bert.gz
# Path to folder with resulting BERT files
resulting_bert_files_path = MLClassificationData/pybert/out
# Type of execution. One of trainAndTest, train, test, crossValidation and none
type_of_execution = trainandtest
# Amount of cross-validation's loops.
cross_validations_total = 2
#cross_validations_total = 10
# Need to save datasets, correpond to cross-validation. loop with the best results
save_cross_validations_datasets = True
# Path to the folder containing train and test datasets used in cross-validation. loop with the best results
cross_validations_datasets_path = MLClassificationData/crossValidation
# Show results of testing
show_test_results = True
# Custom rank threshold
customRank = yes
# minimum probability to predict specific label
rank_threshold = 0.5

[collector]
# Show consolidated results
show_consolidated_results = False
# Calculate and save reports
save_reports = True
# Path to the folder containing reports
reports_path = MLClassificationData/reports
# Prepare resources for runtime
prepare_resources_for_runtime = True
# Path to the folder containing saved resources
saved_resources_path = MLClassificationData/runtime
# Custom consolidated rank threshold
consolidatedRank = yes
# Custom rank threshold (part of models in chain) for consolidated results
consolidated_rank_threshold = 0.5

# === Requests ===
# Request defines the pipe - chain of processes, in which previous processes can prepare data or set parameters
# for subsequent. Processes are separated by symbol '|'. Currently we support 5 types of processes:
# W - word embedding
# P - preprocess
# D - data loading
# M - create/train/test model
# C - collector (consolidate results of model's testing and save resources for runtime)
# Each process has the following structure:
# <Symbol_of_process>(<list_of_parameters)
# List of parameters is a list of configuration's options, which should be changed for current and subsequent
# processes.
[requests]

# Use single doc tokenization
#request = D(load_w2v_model=False; enable_tokenization=True) | M(type=svc; name=svc; type_of_execution=crossvalidation) | C(saveResources=True)
request=D(load_w2v_model=False;enable_tokenization=True)|M(type=svc;name=svc;type_of_execution=crossvalidation)|C()

# Use set of docs tokenization
#request = D(load_w2v_model=False; enable_tokenization=False) | P() | M(type=svc; name=svc; type_of_execution=crossvalidation)

# No tokenization
#request = D(load_w2v_model=False) | M(type=svc; name=svc; type_of_execution=crossvalidation)

#request = P()
#request = D(enable_tokenization=True) | M(type=snn; name=snn; epochs=30) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(load_w2v_model=False) | M(type=cnn; name=cnn; epochs=10) | M(type=perceptron; name=perceptron) | M(type=svc; name=svc) | C()
#request = D(load_w2v_model=False) | M(type=perceptron; name=perceptron; type_of_execution=crossvalidation)
#request = D(load_w2v_model=False; train_data_path=MLClassificationData/crossValidation/train; test_data_path=MLClassificationData/crossValidation/test) | M(type=perceptron; name=perceptron)
info_from = 2 days
# days(s), today
