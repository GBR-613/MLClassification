{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization with Stanford Tokenizer.\n",
    "Please check, that Stanford Server is running:    \n",
    "[Start server and/or check that it is running](http://localhost:8888/notebooks/MLClassification/tokenizer/arabic/stanford/Start%20Stanford%20Server%20from%20Python.ipynb#2)    \n",
    "\n",
    "It is assumed here, that all source text files will be placed in the folder ~/MLClassificationData.    \n",
    "Original and resulting (tokenized) files will have the same names and will be placed in the same file's tree, under the same root folder:\n",
    "- original files - under subfolder _source_, \n",
    "- tokenized files - under subfolder _target_.\n",
    "\n",
    "_**Note:**_ you should change value of variable _root_ to set actual name of the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /home/user/MLClassificationData/test/source/docs.txt (66 lines, 1159 tokens): in 5 sec\n",
      "File /home/user/MLClassificationData/test/source/Fold1/docs.txt (66 lines, 1159 tokens): in 5 sec\n",
      "File /home/user/MLClassificationData/test/source/Fold1/Fold2/docs.txt (66 lines, 1159 tokens): in 5 sec\n",
      "File /home/user/MLClassificationData/test/source/Fold1/zdocs.txt (66 lines, 1159 tokens): in 5 sec\n",
      "File /home/user/MLClassificationData/test/source/zdocs.txt (66 lines, 1159 tokens): in 5 sec\n",
      "Tokenization complited in 25 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "from pathlib import Path\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser(url='http://localhost:9005', tagtype='pos')\n",
    "\n",
    "# Change value of this variable \n",
    "root = \"test\"\n",
    "homePath = str(Path.home()) + \"/MLClassificationData\"\n",
    "inPath = homePath + \"/\" + root + \"/source\"\n",
    "\n",
    "def tokenizeData(path):\n",
    "    curDir = os.getcwd()\n",
    "    os.chdir(path);\n",
    "    for ff in glob.glob(\"*\"):\n",
    "        if os.path.isdir(ff):\n",
    "            dPath = path + \"/\" + ff\n",
    "            tPath = dPath.replace(\"source\",\"target\")\n",
    "            if os.path.exists(tPath):\n",
    "                shutil.rmtree(tPath)\n",
    "            os.mkdir(tPath)\n",
    "            tokenizeData(dPath)\n",
    "            continue\n",
    "        fPath = path + \"/\" + ff\n",
    "        q = 0;        \n",
    "        qt = 0;\n",
    "        ds = datetime.datetime.now()\n",
    "        outPath = fPath.replace(\"source\", \"target\")\n",
    "        outFile = open(outPath, 'w', encoding='UTF-8')\n",
    "        with open(fPath, 'r', encoding='UTF-8') as f:            \n",
    "            for line in f:\n",
    "                q += 1\n",
    "                if q > 1:\n",
    "                    result = '\\n'\n",
    "                else:\n",
    "                    result = ''\n",
    "                line = line.replace('\\r', '').replace('\\n', '')\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                toks = line.split()\n",
    "                print (\"                                                                 \", end='\\r')\n",
    "                print (\"Load %d line, len: %d, toks: %d\"%(q, len(line), len(toks)), end='\\r')               \n",
    "                if len(toks) == 0:\n",
    "                    continue\n",
    "                qt += len(toks)\n",
    "                tArr = parser.tag(line.split())\n",
    "                result += joinTokens(tArr).strip()\n",
    "                outFile.write(result)\n",
    "        de = datetime.datetime.now()\n",
    "        print (\"File %s (%d lines, %d tokens): in %s\"%(fPath, q, qt, showTime(ds, de)))\n",
    "        f.close()\n",
    "        outFile.close()\n",
    "\n",
    "def joinTokens(tArr):\n",
    "    toks = [x[0] for x in tArr]\n",
    "    tags = [x[1] for x in tArr]\n",
    "    result = ''\n",
    "    for i in range(len(tArr)):        \n",
    "        ftok = ''\n",
    "        if i > 0:\n",
    "            result += ' '\n",
    "        if tags[i] == \"DT\" or tags[i] == \"IN\" or tags[i] == \"PUNC\" or tags[i] == \"CD\" or tags[i] == 'PRP$':\n",
    "            continue\n",
    "        else:\n",
    "            if tags[i].startswith(\"DT\"):\n",
    "                ftok = toks[i][2:]\n",
    "            else:\n",
    "                ftok = toks[i]\n",
    "            result += ftok\n",
    "    return result\n",
    "\n",
    "def showTime(ds,de):\n",
    "    result = ''\n",
    "    seconds = (de-ds).total_seconds()\n",
    "    hh = int(seconds/(60*24));\n",
    "    if hh > 0:\n",
    "        result = \"%d h:\"%(hh);\n",
    "    seconds -= hh*60*24\n",
    "    mm = int(seconds/60);\n",
    "    if mm > 0:\n",
    "        result += \"%d min:\"%(mm)\n",
    "    ss = seconds - mm*60;\n",
    "    result += \"%d sec\"%(ss)\n",
    "    return result\n",
    "    \n",
    "fds = datetime.datetime.now()\n",
    "tokenizeData(inPath)\n",
    "fde = datetime.datetime.now()\n",
    "print (\"Tokenization complited in %s\"%(showTime(fds,fde)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
