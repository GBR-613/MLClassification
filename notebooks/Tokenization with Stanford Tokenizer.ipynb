{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization with Stanford Tokenizer.\n",
    "Please check, that Stanford Server is running:    \n",
    "[Start server and/or check that it is running](http://localhost:8888/notebooks/MLClassification/tokenizer/arabic/stanford/Start%20Stanford%20Server%20from%20Python.ipynb#2)    \n",
    "\n",
    "It is assumed here, that all source text files will be placed in the folder ~/MLClassificationData.    \n",
    "Original and resulting (tokenized) files will have the same names and will be placed in the same file's tree, under the same root folder:\n",
    "- original files - under subfolder _source_, \n",
    "- tokenized files - under subfolder _target_.\n",
    "\n",
    "_**Note:**_ you should change value of variable _root_ to set actual name of the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "from pathlib import Path\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser(url='http://localhost:9005', tagtype='pos')\n",
    "\n",
    "# Change value of this variable \n",
    "root = \"test\"\n",
    "homePath = str(Path.home()) + \"/MLClassificationData\"\n",
    "inPath = homePath + \"/\" + root + \"/source\"\n",
    "\n",
    "def tokenizeData(path):\n",
    "    curDir = os.getcwd()\n",
    "    os.chdir(path);\n",
    "    for ff in glob.glob(\"*\"):\n",
    "        if os.path.isdir(ff):\n",
    "            dPath = path + \"/\" + ff\n",
    "            tPath = dPath.replace(\"source\",\"target\")\n",
    "            if os.path.exists(tPath):\n",
    "                shutil.rmtree(tPath)\n",
    "            os.mkdir(tPath)\n",
    "            tokenizeData(dPath)\n",
    "            continue\n",
    "        fPath = path + \"/\" + ff\n",
    "        result = ''\n",
    "        q = 0;        \n",
    "        qt = 0;\n",
    "        ds = datetime.datetime.now()\n",
    "        with open(fPath, 'r', encoding='UTF-8') as f:            \n",
    "            for line in f:\n",
    "                q += 1\n",
    "                line = line.replace('\\r', '').replace('\\n', '')\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                toks = line.split()\n",
    "                print (\"                                                                 \", end='\\r')\n",
    "                print (\"Load %d line, len: %d, toks: %d\"%(q, len(line), len(toks)), end='\\r')               \n",
    "                if len(toks) == 0:\n",
    "                    continue\n",
    "                qt += len(toks)\n",
    "                tArr = parser.tag(line.split())\n",
    "                result += joinTokens(tArr).strip() + \"\\n\"\n",
    "        de = datetime.datetime.now()\n",
    "        print (\"File %s (%d lines, %d tokens) is tokenized in %d sec.\"%(fPath, q, qt, (de-ds).total_seconds()))\n",
    "        f.close()\n",
    "        outPath = fPath.replace(\"source\", \"target\")\n",
    "        outFile = open(outPath, 'w', encoding='UTF-8')\n",
    "        outFile.write(result.strip())\n",
    "        outFile.close()\n",
    "\n",
    "def joinTokens(tArr):\n",
    "    toks = [x[0] for x in tArr]\n",
    "    tags = [x[1] for x in tArr]\n",
    "    result = ''\n",
    "    for i in range(len(tArr)):        \n",
    "        ftok = ''\n",
    "        if tags[i] == \"DT\" or tags[i] == \"IN\" or tags[i] == \"PUNC\" or tags[i] == \"CD\" or tags[i] == 'PRP$':\n",
    "            continue\n",
    "        else:\n",
    "            if tags[i].startswith(\"DT\"):\n",
    "                ftok = toks[i][2:]\n",
    "            else:\n",
    "                ftok = toks[i]\n",
    "            result += ftok\n",
    "    return result\n",
    "\n",
    "fds = datetime.datetime.now()\n",
    "tokenizeData(inPath)\n",
    "fde = datetime.datetime.now()\n",
    "print (\"Tokenization complited in %d sec\"%((fde-fds).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
