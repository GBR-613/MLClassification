{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declarations and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import gensim\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import re\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from keras.models import load_model\n",
    "import statistics\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "homePath = str(Path.home()) + \"/MLClassificationData\"\n",
    "#homePath = \"c:/BERT/rtanews/docs/\"\n",
    "LabeledSentence = gensim.models.doc2vec.TaggedDocument\n",
    "# Count of categories (labels).\n",
    "n_cats = 0\n",
    "# This id is included into the names of models, created by this notebook.\n",
    "modelId = 5\n",
    "# Path to original corpus for training\n",
    "trainRoot = homePath + '/train/rtanews/source'\n",
    "# Path to original corpus for testing\n",
    "testRoot = homePath + '/test/rtanews/source'\n",
    "# Folder containing pretrained model from google\n",
    "bert_ckpt_path = homePath + \"/BERT/googlebert/\"\n",
    "# Input for BERT\n",
    "bertDataPath = homePath + \"/PytorchBERT/textdir/\"\n",
    "# Output of BERT\n",
    "outDataPath = homePath + \"/PytorchBERT/outdir/\"\n",
    "# Path to model info\n",
    "modelInfoPath = homePath + '/models/rtanews/modelinfo/'\n",
    "max_bert_seq_length = 512\n",
    "numpy.random.seed(1)\n",
    "cnt = 0\n",
    "nCats = 0\n",
    "categories = dict()\n",
    "\n",
    "LabeledDocument = namedtuple('LabeledDocument', 'line words labels qLabs name')\n",
    "\n",
    "def getCategories(path):\n",
    "    cats = dict()\n",
    "    nCats = 0\n",
    "    os.chdir(path)\n",
    "    for f in glob.glob(\"*\"):\n",
    "        if os.path.isdir(f):\n",
    "            cats[f] = nCats\n",
    "            nCats += 1\n",
    "    return cats\n",
    "\n",
    "def prepareDocsData(path, cats):\n",
    "    files = dict()\n",
    "    fInCats = [0] * len(cats)\n",
    "    nFiles = 0\n",
    "    actFiles = 0\n",
    "    curCategory = 0\n",
    "    docs = []\n",
    "    os.chdir(path)\n",
    "    rootDir = os.getcwd()\n",
    "    for f in glob.glob(\"*\"):\n",
    "        curCategory = cats[f]\n",
    "        catPath = path + \"/\" + f\n",
    "        os.chdir(catPath)\n",
    "        for fc in glob.glob(\"*\"):\n",
    "            actFiles += 1\n",
    "            if fc not in files:\n",
    "                nFiles += 1\n",
    "                fPath = catPath + \"/\" + fc\n",
    "                docCont = ''\n",
    "                with open(fc, 'r', encoding='UTF-8') as tc:\n",
    "                    for line in tc:\n",
    "                        docCont += line.strip() + \" \"\n",
    "                tc.close()\n",
    "                line = docCont.strip() + \"\\n\"\n",
    "                words = docCont.strip().split()\n",
    "                labels = [0] * len(cats)\n",
    "                labels[curCategory] = 1\n",
    "                files[fc] = LabeledDocument(line, words, labels, [1], fc)\n",
    "            else:\n",
    "                files[fc].labels[curCategory] = 1\n",
    "                files[fc].qLabs[0] += 1\n",
    "            fInCats[curCategory] += 1\n",
    "    for k, val in files.items():\n",
    "        docs.append(val)\n",
    "    return docs, fInCats\n",
    "\n",
    "def getLabelSets(docs):\n",
    "    labels = [x[2] for x in docs]\n",
    "    results = [labels[0]]\n",
    "    qLabs = 0\n",
    "    for i in range(len(labels)):        \n",
    "        if i%1000 == 0:\n",
    "            print (str(i), end='\\r')\n",
    "        qLabs += sum(labels[i])\n",
    "        count = 0\n",
    "        for j in range(len(results)):\n",
    "            for k in range(len(categories)):\n",
    "                if labels[i][k] != results[j][k]:\n",
    "                    count += 1\n",
    "                    break\n",
    "        if count == len(results):\n",
    "            results.append(labels[i])\n",
    "    return len(results), qLabs\n",
    "    \n",
    "def showTime(ds,de):\n",
    "    result = ''\n",
    "    seconds = (de-ds).total_seconds()\n",
    "    if seconds < 1:\n",
    "        return \"less than 1 sec\"\n",
    "    hh = int(seconds/(60*60));\n",
    "    if hh > 0:\n",
    "        result = \"%d h:\"%(hh);\n",
    "    seconds -= hh*60*60\n",
    "    mm = int(seconds/60);\n",
    "    if mm > 0:\n",
    "        result += \"%d min:\"%(mm)\n",
    "    ss = seconds - mm*60;\n",
    "    result += \"%d sec\"%(ss)\n",
    "    return result\n",
    "\n",
    "def showDocsByLength(plt):\n",
    "    fig, (plot1, plot2) = plt.subplots(1, 2, figsize=(10,6))    \n",
    "    dictLens = dict()\n",
    "    dictLens1 = dict()\n",
    "    for i in range(len(trainDocs)):\n",
    "        lend = \"%5d\"%(len(trainDocs[i].words))\n",
    "        if not lend in dictLens:\n",
    "            dictLens[lend] = 1\n",
    "        else:\n",
    "            dictLens[lend] += 1\n",
    "    lens = sorted(list(dictLens.items()))\n",
    "    lvars = [int(x[0]) for x in lens]\n",
    "    locc = [x[1] for x in lens]\n",
    "    plot1.set_title (\"Documents by length in training set\")\n",
    "    plot1.set_ylabel(\"Documents\")\n",
    "    plot1.set_xlabel(\"Length\")\n",
    "    plot1.plot(lvars, locc, \"b.-\") \n",
    "    for i in range(len(testDocs)):\n",
    "        lend = \"%5d\"%(len(testDocs[i].words))\n",
    "        if not lend in dictLens1:\n",
    "            dictLens1[lend] = 1\n",
    "        else:\n",
    "            dictLens1[lend] += 1\n",
    "    lens1 = sorted(list(dictLens1.items()))\n",
    "    lvars1 = [int(x[0]) for x in lens1]\n",
    "    locc1 = [x[1] for x in lens1]\n",
    "    plot2.set_title (\"Documents by length in testing set\")\n",
    "    plot2.set_xlabel(\"Length\")\n",
    "    plot2.yaxis.tick_right()\n",
    "    plot2.plot(lvars1, locc1, \"b.-\") \n",
    "    plt.show()\n",
    "    \n",
    "def showDocsByLabs(plt):\n",
    "    fig, (plot1, plot2) = plt.subplots(1, 2, figsize=(10,6))\n",
    "    dictLabs = dict()\n",
    "    dictLabs1 = dict()\n",
    "    for i in range(len(trainDocs)):\n",
    "        lab = \"%5d\"%(trainDocs[i].qLabs[0])\n",
    "        if not lab in dictLabs:\n",
    "            dictLabs[lab] = 1\n",
    "        else:\n",
    "            dictLabs[lab] += 1\n",
    "    labs = sorted(list(dictLabs.items()))\n",
    "    lvars1 = [int(x[0]) for x in labs]\n",
    "    locc1 = [x[1] for x in labs]\n",
    "    plot1.set_title (\"Documents by labels in training set\")\n",
    "    plot1.set_ylabel(\"Documents\")\n",
    "    plot1.set_xlabel(\"Labels\")\n",
    "    plot1.set_xticks(numpy.arange(0, len(categories), step=1))\n",
    "    plot1.plot(lvars1, locc1, \"bo-\")\n",
    "    for i in range(len(testDocs)):\n",
    "        lab = \"%5d\"%(testDocs[i].qLabs[0])\n",
    "        if not lab in dictLabs1:\n",
    "            dictLabs1[lab] = 1\n",
    "        else:\n",
    "            dictLabs1[lab] += 1\n",
    "    labs1 = sorted(list(dictLabs1.items()))\n",
    "    lvars2 = [int(x[0]) for x in labs1]\n",
    "    locc2 = [x[1] for x in labs1]\n",
    "    plot2.set_title (\"Documents by labels in testing set\")\n",
    "    #plot2.set_ylabel(\"Documents\")\n",
    "    plot2.set_xlabel(\"Labels\")\n",
    "    plot2.set_xticks(numpy.arange(0, len(categories), step=1))\n",
    "    plot2.yaxis.tick_right()\n",
    "    plot2.plot(lvars2, locc2, \"bo-\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datetime.datetime.now()\n",
    "categories = getCategories(trainRoot)\n",
    "#print (categories)\n",
    "trainAllDocs, fInCats1 = prepareDocsData(trainRoot, categories)\n",
    "trainAllDocs = random.sample(trainAllDocs, len(trainAllDocs))\n",
    "#trainDocs = trainAllDocs[:int(len(trainAllDocs) * (1 - valPart))]\n",
    "#valDocs = trainAllDocs[int(len(trainAllDocs) * (1 - valPart)):]\n",
    "trainDocs = trainAllDocs\n",
    "testDocs, fInCats2 = prepareDocsData(testRoot, categories)\n",
    "#testDocs = random.sample(testDocs, len(testDocs))\n",
    "de = datetime.datetime.now()\n",
    "\n",
    "print (\"Load input data in %s\"%(showTime(ds, de))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Dataset properties:\")\n",
    "maxDocLen = max(len(x.words) for x in trainAllDocs)\n",
    "minDocLen = min(len(x.words) for x in trainAllDocs)\n",
    "avrgDocLen = round(statistics.mean(len(x.words) for x in trainAllDocs), 2)\n",
    "medianDocLen = round(statistics.median(len(x.words) for x in trainAllDocs), 2)\n",
    "dls, qLabs = getLabelSets(trainAllDocs)\n",
    "\n",
    "print ('Loaded %d documents: %d for training, %d for test' % (len(trainAllDocs) + len(testDocs), len(trainDocs), len(testDocs)))\n",
    "print (\"Length of documents: maximum: %d, minimum: %d, average: %d, median: %d\"%(maxDocLen, minDocLen, avrgDocLen, medianDocLen))\n",
    "showDocsByLength(plt);\n",
    "\n",
    "top_bound=0.9\n",
    "maxLen = math.ceil(maxDocLen/100)*100 + 100\n",
    "input_length_list=[]\n",
    "for i in range(100, maxLen, 100):\n",
    "    input_length_list.append(i)\n",
    "input_length_dict={x:0 for x in input_length_list }\n",
    "for i in range(len(trainAllDocs)):\n",
    "    curLen = len(trainAllDocs[i].words)\n",
    "    dictLen = maxLen\n",
    "    for ln in input_length_dict:\n",
    "        if curLen < ln:\n",
    "            dicLen = ln\n",
    "            break\n",
    "    input_length_dict[dicLen] = input_length_dict[dicLen] + 1\n",
    "input_length_dict_percentage={}\n",
    "for k,v in input_length_dict.items():\n",
    "    v=v/len(trainAllDocs)\n",
    "    input_length_dict_percentage[k]=v\n",
    "maxSeqLength=0\n",
    "accumulate_percentage=0\n",
    "for length,percentage in input_length_dict_percentage.items():\n",
    "    accumulate_percentage+=percentage\n",
    "    if accumulate_percentage>0.9:\n",
    "        maxSeqLength=length\n",
    "        break\n",
    "print (\"Length of %.1f%% documents from training set is less then %d. Longer documents will be truncated.\"%(top_bound*100, maxSeqLength))        \n",
    "\n",
    "print ()\n",
    "print (\"Categories (labels): %d\"%(len(categories)))\n",
    "print (\"Documents for training in category : maximum: %d, minimum: %d, avegare: %d\"%(max(fInCats1), min(fInCats1), round(statistics.mean(fInCats1), 2)))\n",
    "print (\"Documents for testing  in category : maximum: %d, minimum: %d, avegare: %d\"%(max(fInCats2), min(fInCats2), round(statistics.mean(fInCats2), 2)))\n",
    "showDocsByLabs(plt)\n",
    "print (\"Distinct Label Set: %d\"%(dls))\n",
    "print (\"Proportion of Distinct Label Set: %.4f\"%(dls/len(trainAllDocs)))\n",
    "print (\"Label Cardinality: %.4f\"%(qLabs/len(trainAllDocs)))\n",
    "print (\"Label Density: %.4f\"%(qLabs/len(trainAllDocs)/len(categories)))\n",
    "\n",
    "#del trainAllDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare input for training and testing BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cNames = [''] * len(categories)\n",
    "for k,v in categories.items():\n",
    "    cNames[v] = k\n",
    "    \n",
    "def saveData(type):\n",
    "    global bertDataPath\n",
    "    if type == \"train\":\n",
    "        bertPath = bertDataPath + \"train.tsv\"\n",
    "        data = trainDocs        \n",
    "    else:\n",
    "        bertPath = bertDataPath + \"dev.tsv\"\n",
    "        data = testDocs        \n",
    "    target = open(bertPath, \"w\", encoding=\"utf-8\")\n",
    "    for i in range(len(data)):\n",
    "        conts = data[i].line.replace('\\r','').replace('\\n','.')\n",
    "        labs = []\n",
    "        for j in range(len(data[i].labels)):\n",
    "            if data[i].labels[j] == 1:\n",
    "                #labs.append(cNames[j].replace(\".\",\"\"))\n",
    "                labs.append(cNames[j])\n",
    "        nl = '\\n'\n",
    "        if i == 0:\n",
    "            nl = ''\n",
    "        string = nl + \",\".join(labs) + \"\\t\" + conts\n",
    "        target.write(string)\n",
    "    target.close()    \n",
    "\n",
    "ds = datetime.datetime.now()\n",
    "saveData(\"train\")\n",
    "print (\"Train data saved\")\n",
    "saveData(\"test\")\n",
    "de = datetime.datetime.now()\n",
    "print (\"Test data saved\")\n",
    "print (\"Done in %s\"%(showTime(ds, de))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert google pre-trained model into pytorch model\n",
    "\n",
    "Before launch this cell you need download pre-trained BERT Multi-lingual model from the following link: https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip and unzip it into the folder bert_ckpt_path.     \n",
    "**You need launch this cell only once.** Pytorch model (**pytorch_bert.bin**) will be created in the same folder.\n",
    "\n",
    "After this you should zip all contents of this folder in gz format. Resulting file (**pytorch_bert.gz**) should be placed into ~/MLClassficationData directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForPreTraining\n",
    "\n",
    "def load_tf_weights_in_bert(model, tf_checkpoint_path):\n",
    "    \"\"\" Load tf checkpoints in a pytorch model\n",
    "    \"\"\"\n",
    "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
    "    print(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
    "    # Load weights from TF model\n",
    "    init_vars = tf.train.list_variables(tf_path)\n",
    "    names = []\n",
    "    arrays = []\n",
    "    for name, shape in init_vars:\n",
    "        #print(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
    "        array = tf.train.load_variable(tf_path, name)\n",
    "        names.append(name)\n",
    "        arrays.append(array)\n",
    "\n",
    "    for name, array in zip(names, arrays):\n",
    "        name = name.split('/')\n",
    "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
    "        # which are not required for using pretrained model\n",
    "        if any(n in [\"adam_v\", \"adam_m\"] for n in name):\n",
    "            print(\"Skipping {}\".format(\"/\".join(name)))\n",
    "            continue\n",
    "        pointer = model\n",
    "        for m_name in name:\n",
    "            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n",
    "                l = re.split(r'_(\\d+)', m_name)\n",
    "            else:\n",
    "                l = [m_name]\n",
    "            if l[0] == 'kernel' or l[0] == 'gamma':\n",
    "                pointer = getattr(pointer, 'weight')\n",
    "            elif l[0] == 'output_bias' or l[0] == 'beta':\n",
    "                pointer = getattr(pointer, 'bias')\n",
    "            elif l[0] == 'output_weights':\n",
    "                pointer = getattr(pointer, 'weight')\n",
    "            else:\n",
    "                pointer = getattr(pointer, l[0])\n",
    "            if len(l) >= 2:\n",
    "                num = int(l[1])\n",
    "                pointer = pointer[num]\n",
    "        if m_name[-11:] == '_embeddings':\n",
    "            pointer = getattr(pointer, 'weight')\n",
    "        elif m_name == 'kernel':\n",
    "            array = numpy.transpose(array)\n",
    "        try:\n",
    "            assert pointer.shape == array.shape\n",
    "        except AssertionError as e:\n",
    "            e.args += (pointer.shape, array.shape)\n",
    "            raise\n",
    "        #print(\"Initialize PyTorch weight {}\".format(name))\n",
    "        pointer.data = torch.from_numpy(array)\n",
    "    return model\n",
    "\n",
    "def convert_tf_checkpoint_to_pytorch(tf_checkpoint, bert_config, pytorch_dump):\n",
    "    global bert_ckpt_path\n",
    "    tf_checkpoint_path = bert_ckpt_path + tf_checkpoint\n",
    "    bert_config_file = bert_ckpt_path + bert_config\n",
    "    pytorch_dump_path = bert_ckpt_path + pytorch_dump\n",
    "    # Initialise PyTorch model\n",
    "    config = BertConfig.from_json_file(bert_config_file)\n",
    "    print(\"Building PyTorch model from configuration: {}\".format(bert_config_file))\n",
    "    model = BertForPreTraining(config)\n",
    "\n",
    "    # Load weights from tf checkpoint\n",
    "    load_tf_weights_in_bert(model, tf_checkpoint_path)\n",
    "\n",
    "    # Save pytorch-model\n",
    "    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
    "    torch.save(model.state_dict(), pytorch_dump_path)\n",
    "\n",
    "ds = datetime.datetime.now()    \n",
    "convert_tf_checkpoint_to_pytorch('bert_model.ckpt', 'bert_config.json', 'pytorch_model.bin')\n",
    "de = datetime.datetime.now()\n",
    "print (\"Google model converted in %s\"%(showTime(ds, de))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class which performs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from pytorch_pretrained_bert.modeling import PreTrainedBertModel\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "class BertForMultiLabelSequenceClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create, train, save and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) [string]. The labels of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "class DataProcessor(object):\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        labels_list = []\n",
    "        cNames = [''] * len(categories)\n",
    "        for k,v in categories.items():\n",
    "            labels_list.append(k)\n",
    "        return labels_list\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[1]\n",
    "            text_b = None\n",
    "            label = line[0]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = [0] * len(categories)\n",
    "        exLabels = example.label.split(\",\")\n",
    "        for i in range(len(exLabels)):\n",
    "            label_id[label_map[exLabels[i]]] = 1\n",
    "            \n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n",
    "def accuracy(y_pred, y_true, thresh:float=0.5):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
    "\n",
    "max_bert_seq_length = 512\n",
    "class Args(object):\n",
    "    def __init__(self, bert_model):\n",
    "        self.bert_model = bert_model\n",
    "    \n",
    "\n",
    "args = Args(bert_ckpt_path + \"pytorch_model.bin\")\n",
    "args.data_dir = bertDataPath\n",
    "args.bert_model = homePath + \"/pytorch_bert.gz\"\n",
    "args.output_dir = outDataPath\n",
    "args.max_seq_length = min(max_bert_seq_length, maxSeqLength)\n",
    "args.do_train = True\n",
    "args.do_eval = True\n",
    "args.do_lower_case = False\n",
    "args.train_batch_size = 32\n",
    "args.eval_batch_size = 8\n",
    "args.learning_rate = 5e-5\n",
    "args.num_train_epochs = 3\n",
    "args.warmup_proportion = 0.1\n",
    "args.no_cuda = True\n",
    "args.local_rank = -1\n",
    "args.seed = 42\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "device = 'cpu'\n",
    "n_gpu = torch.cuda.device_count()\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if not args.do_train and not args.do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "processor = DataProcessor()\n",
    "num_labels = len(categories)\n",
    "label_list = processor.get_labels()\n",
    "#tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "tokenizer = BertTokenizer(bert_ckpt_path + 'vocab.txt')\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None    \n",
    "if args.do_train:\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    num_train_optimization_steps = int(\n",
    "        len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "model = BertForMultiLabelSequenceClassification.from_pretrained(args.bert_model,\n",
    "    cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(args.local_rank),\n",
    "    num_labels = num_labels)\n",
    "model.to(device)\n",
    "\n",
    "#namedParams = [p for p in model.named_parameters()]\n",
    "#print (\"!!! model.named_parameters():\")\n",
    "#print (namedParams)\n",
    "#param_optimizer = list(model.named_parameters())\n",
    "param_optimizer = [p for p in model.named_parameters()]\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "    lr=args.learning_rate,\n",
    "    warmup=args.warmup_proportion,\n",
    "    t_total=num_train_optimization_steps)\n",
    "\n",
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0\n",
    "\n",
    "if args.do_train:\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    model.train()\n",
    "    for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(args.output_dir, \"pytorch_model.bin\")\n",
    "if args.do_train:\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file)\n",
    "model = BertForMultiLabelSequenceClassification.from_pretrained(args.bert_model, state_dict=model_state_dict, num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    allLabs = None\n",
    "    res = None\n",
    "    initRes = True\n",
    "    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "        preds = logits.sigmoid().to('cpu').numpy()\n",
    "        labs = label_ids.to('cpu').numpy()\n",
    "        if initRes == True:\n",
    "            res = preds\n",
    "            allLabs = labs\n",
    "            initRes = False\n",
    "        else:\n",
    "            res = numpy.concatenate((res, preds))\n",
    "            allLabs = numpy.concatenate((allLabs, labs))\n",
    "\n",
    "        tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    loss = tr_loss/nb_tr_steps if args.do_train else None\n",
    "    result = {'eval_loss': eval_loss,\n",
    "        'eval_accuracy': eval_accuracy,\n",
    "        'global_step': global_step,\n",
    "        'loss': loss}\n",
    "    \n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    writer.close()\n",
    "\n",
    "    output_pred_file = os.path.join(args.output_dir, \"predictions.txt\")\n",
    "    with open(output_pred_file, \"w\") as writer:\n",
    "        for i in range(len(res)):\n",
    "            line = \"\"\n",
    "            for j in range(len(allLabs[i])):\n",
    "                if allLabs[i][j] == 1:\n",
    "                    if line != '':\n",
    "                        line = line + \",\"\n",
    "                    line = line + str(j)\n",
    "            for j in range(len(res[i])):\n",
    "                line = line + \"\\t\"\n",
    "                line = line + str(res[i][j])\n",
    "            line = line + \"\\n\"\n",
    "            writer.write(line)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Calculate metrics.\")\n",
    "rankThreshold = 0.3\n",
    "test_labels = allLabs\n",
    "\n",
    "print(\"Len of test_labels: %d, results: %d\"%(len(test_labels), len(res)))\n",
    "\n",
    "def rankIndicator(labels, predictions, index):\n",
    "    global rankThreshold\n",
    "    actual = labels[index] == 1\n",
    "    predicted = predictions[index] >= rankThreshold\n",
    "    notActual = 0\n",
    "    if actual == True and predicted == False:        \n",
    "        for i in range(len(predictions)):\n",
    "            if i == index:\n",
    "                continue\n",
    "            if labels[i] == 0:\n",
    "                notActual += 1\n",
    "            if labels[i] == 0 and ((predictions[index] / predictions[i]) < diffThreshold):\n",
    "                return True, False\n",
    "        if notActual > 0:\n",
    "            return True, True\n",
    "    return actual, predicted\n",
    "    \n",
    "def getPrediction(entry):\n",
    "    return entry[1]\n",
    "\n",
    "# General results   \n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "qTags = 0\n",
    "qPreds = 0\n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if predicted:\n",
    "            qPreds += 1\n",
    "            if actual:\n",
    "                qTags += 1\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if actual:\n",
    "                qTags += 1\n",
    "                fn += 1\n",
    "print (\"Labels actual: %d, predicted: %d, correctly: %d, incorrectly: %d, not predicted: %d\"%(qTags, qPreds, tp, fp, fn))\n",
    "\n",
    "#Exact Match Ratio\n",
    "wrongPreds = 0\n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if (actual and not predicted) or (predicted and not actual):\n",
    "            wrongPreds += 1\n",
    "            break;\n",
    "emr = (len(test_labels) - wrongPreds)/len(test_labels)           \n",
    "print (\"Exact Match Ratio:  %.2f%%\" % ((len(test_labels) - wrongPreds)/len(test_labels) * 100))\n",
    "\n",
    "#Accuracy\n",
    "accuracy = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "        if predicted and not actual:\n",
    "            tfp += 1\n",
    "    accuracy += tp / (labels + tfp)\n",
    "modelAccuracy = accuracy / len(test_labels)\n",
    "print (\"Accuracy:  %.2f%%\" % (accuracy / len(test_labels) * 100))  \n",
    "\n",
    "#Precision\n",
    "precision = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "    precision += tp / labels\n",
    "modelPrecision = precision / len(test_labels)\n",
    "print (\"Precision:  %.2f%%\" % (precision / len(test_labels) * 100))  \n",
    "\n",
    "#Recall\n",
    "recall = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "        if predicted:\n",
    "            tfp += 1\n",
    "    if tfp > 0:\n",
    "        recall += tp / tfp\n",
    "modelRecall = recall / len(test_labels)      \n",
    "print (\"Recall:  %.2f%%\" % (recall / len(test_labels) * 100))  \n",
    "\n",
    "#F1-Measure\n",
    "modelF1 = 2 * (modelPrecision * modelRecall / (modelPrecision + modelRecall))\n",
    "print (\"F1-Measure:  %.2f%%\" % (2 * (modelPrecision * modelRecall / (modelPrecision + modelRecall)) * 100))\n",
    "\n",
    "#Hamming Loss\n",
    "hl = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if (actual and not predicted) or (predicted and not actual):\n",
    "            hl += 1\n",
    "modelHl = hl / (len(test_labels) * len(categories))           \n",
    "print (\"Hamming Loss:  %.2f%%\" % (hl * 100 / (len(test_labels) * len(categories)))) \n",
    "\n",
    "#Macro-Averaged Precision\n",
    "precision = 0\n",
    "for i in range(len(categories)):\n",
    "    tp = 0\n",
    "    tact = 0\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i) \n",
    "        if not actual:\n",
    "            continue\n",
    "        tact += 1\n",
    "        if predicted:\n",
    "            tp += 1\n",
    "    precision += tp / tact\n",
    "modelMacroPrecision = precision / len(categories) \n",
    "print (\"Macro-Averaged Precision:  %.2f%%\" % (precision / len(categories) * 100))  \n",
    "\n",
    "#Macro-Averaged Recall\n",
    "recall = 0\n",
    "for i in range(len(categories)):\n",
    "    tp = 0\n",
    "    tact = 0\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "    if tact > 0:                \n",
    "        recall += tp / tact\n",
    "    #else:\n",
    "    #    print (\"Macro-Recall: category %d isn't predicted\"%(i))\n",
    "modelMacroRecall = recall / len(categories)\n",
    "print (\"Macro-Averaged Recall:  %.2f%%\" % (recall / len(categories) * 100))  \n",
    "\n",
    "#Macro-Averaged F1-Measure\n",
    "f1 = 0\n",
    "for i in range(len(categories)):\n",
    "    tp = 0\n",
    "    tact = 0\n",
    "    labs = 0\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if actual:\n",
    "            labs += 1\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "    f1 += 2 * tp / (tact + labs)\n",
    "modelMacroF1 = f1 / len(categories)\n",
    "print (\"Macro-Averaged F1-Measure:  %.2f%%\" % (f1 / len(categories) * 100))  \n",
    "\n",
    "#Micro-Averaged Precision\n",
    "precision = 0\n",
    "tp = 0\n",
    "tact = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i) \n",
    "        if not actual:\n",
    "            continue\n",
    "        tact += 1\n",
    "        if predicted:\n",
    "            tp += 1\n",
    "precision += tp / tact\n",
    "modelMicroPrecision = precision\n",
    "print (\"Micro-Averaged Precision:  %.2f%%\" % (precision * 100))  \n",
    "\n",
    "#Micro-Averaged Recall\n",
    "recall = 0\n",
    "tp = 0\n",
    "tact = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "recall += tp / tact\n",
    "modelMicroRecall = recall\n",
    "print (\"Micro-Averaged Recall:  %.2f%%\" % (recall * 100))  \n",
    "\n",
    "#Micro-Averaged F1-Measure\n",
    "f1 = 0\n",
    "tp = 0\n",
    "tact = 0\n",
    "labs = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if actual:\n",
    "            labs += 1\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "f1 += 2 * tp / (tact + labs)\n",
    "modelMicroF1 = f1\n",
    "print (\"Micro-Averaged F1-Measure:  %.2f%%\" % (f1 * 100))  \n",
    "\n",
    "#One error\n",
    "o_err = 0\n",
    "for i in range(len(test_labels)):\n",
    "    list = [(0,0) for i in range(len(categories))]\n",
    "    for j in range(len(categories)):\n",
    "        list[j] = (test_labels[i][j], res[i][j])\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    if list[0][0] == 0:\n",
    "        o_err += 1\n",
    "print (\"One Error: %.2f%%\" % (o_err / len(test_labels) * 100))\n",
    "\n",
    "#Coverage\n",
    "stepsDown = 0\n",
    "for i in range(len(test_labels)):\n",
    "    bound = sum(test_labels[i]) - 1\n",
    "    list = [(0,0,0) for i in range(len(categories))]\n",
    "    for j in range(len(categories)):\n",
    "        list[j] = (test_labels[i][j], res[i][j], j)\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    eSteps = 0\n",
    "    for j in range(len(categories)):\n",
    "        if test_labels[i][j] == 0:\n",
    "            continue\n",
    "        for k in range(len(list)):\n",
    "            if list[k][2] == j:\n",
    "                eSteps = max(eSteps, k)\n",
    "    stepsDown += max(0, eSteps - bound)\n",
    "print (\"Coverage: %.2f\" % (stepsDown / len(test_labels))) \n",
    "\n",
    "#Ranking Loss\n",
    "rl = 0\n",
    "for i in range(len(test_labels)):\n",
    "    mult = sum(test_labels[i])\n",
    "    list = [(0,0) for i in range(len(categories))]\n",
    "    wrongOrder = 0\n",
    "    for j in range(len(categories)):\n",
    "        list[j] = (test_labels[i][j], res[i][j])\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    for j in range(len(list)):\n",
    "        if list[j][0] == 1:\n",
    "            mult -= 1\n",
    "            if mult == 0:\n",
    "                break\n",
    "            continue\n",
    "        wrongOrder += mult\n",
    "    rl += wrongOrder / sum(test_labels[i])\n",
    "print (\"Ranking Loss: %.2f\" % (rl / len(test_labels)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "modelName = \"BERT-PYTORCH-%s\"%(datetime.datetime.now().strftime(\"%Y-%b-%d-%H%M%S\"))\n",
    "DocInfo = namedtuple('DocInfo', 'name actLabs predLabs dtype')\n",
    "CategoryInfo = namedtuple('CategoryInfo', 'name actLabs predLabs wrongLabs notPredLabs qtyDocs qtyPredDocs precision recall f1')\n",
    "ModelInfo = namedtuple('ModelInfo', 'name dataSet cats qtyDocs qtyPredDocs qtyPartDocs qtyWrongDocs actLabs predLabs wrongLabs notPredLabs emr accuracy precision recall f1 hl macroPrecision macroRecall macroF1 microPrecision microRecall microF1 catagories docs')\n",
    "\n",
    "cNames = [''] * len(categories)\n",
    "for k,v in categories.items():\n",
    "    cNames[v] = k\n",
    "mCorrDocs = 0\n",
    "mPartDocs = 0\n",
    "mWrongDocs = 0\n",
    "cqTags = [0] * len(categories)\n",
    "cqPreds = [0] * len(categories)\n",
    "ctp = [0] * len(categories)\n",
    "cfp = [0] * len(categories)\n",
    "cfn = [0] * len(categories)\n",
    "cDocs = [0] * len(categories)\n",
    "cpDocs = [0] * len(categories)\n",
    "\n",
    "docsInfo = []\n",
    "\n",
    "for i in range(len(res)):\n",
    "    qTags = 0\n",
    "    qPreds = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    dtype = 0\n",
    "    actLabs = []\n",
    "    predLabs = []\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual:\n",
    "            cDocs[j] += 1\n",
    "            actLabs.append(cNames[j])\n",
    "        if predicted:\n",
    "            qPreds += 1\n",
    "            cqPreds[j] += 1\n",
    "            predLabs.append(cNames[j])\n",
    "            if actual:\n",
    "                qTags += 1\n",
    "                tp += 1\n",
    "                cqTags[j] += 1\n",
    "                ctp[j] += 1\n",
    "                cpDocs[j] += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "                cfp[j] += 1\n",
    "        else:\n",
    "            if actual:\n",
    "                qTags += 1\n",
    "                fn += 1\n",
    "                cqTags[j] += 1\n",
    "                cfn[j] += 1\n",
    "    if tp == qTags:\n",
    "        if qPreds == tp:\n",
    "            dtype = 2\n",
    "        else:\n",
    "            dtype = 3\n",
    "        mCorrDocs += 1\n",
    "    elif tp > 0:\n",
    "        dtype = 1\n",
    "        mPartDocs += 1\n",
    "    else:\n",
    "        mWrongDocs += 1\n",
    "    docInfo = DocInfo(testDocs[i].name, actLabs, predLabs, dtype)\n",
    "    docsInfo.append(docInfo)\n",
    "\n",
    "categoriesInfo = []\n",
    "for i in range(len(categories)):\n",
    "    cPrec = ctp[i] / cqTags[i]\n",
    "    cRec = 0\n",
    "    if (ctp[i] + cfp[i]) > 0:\n",
    "        cRec = ctp[i] / (ctp[i] + cfp[i])\n",
    "    cF1 = 2 * ctp[i] / (ctp[i] + cfp[i] + cqTags[i])\n",
    "    catInfo = CategoryInfo(cNames[i], cqTags[i], ctp[i], cfp[i], cfn[i], cDocs[i], cpDocs[i], cPrec, cRec, cF1)\n",
    "    categoriesInfo.append(catInfo);\n",
    "\n",
    "sourceRoot = testRoot\n",
    "modelInfo = ModelInfo(modelName,sourceRoot,len(categories),len(res), mCorrDocs, mPartDocs, mWrongDocs, \n",
    "                      sum(cqTags),sum(ctp),sum(cfp),sum(cfn),emr,modelAccuracy,\n",
    "                      modelPrecision,modelRecall,modelF1,modelHl,modelMacroPrecision,modelMacroRecall,\n",
    "                      modelMacroF1,modelMicroPrecision,modelMicroRecall,modelMicroF1,categoriesInfo,docsInfo)\n",
    "with open(modelInfoPath + modelName, 'wb') as f:\n",
    "    pickle.dump(modelInfo, f)\n",
    "print (\"Model info saved.\");\n",
    "print (\"Name: %s, data set: %s, documents: %d, categories: %d, actual labels: %d\"%(modelInfo.name, modelInfo.dataSet, modelInfo.qtyDocs, modelInfo.cats, modelInfo.actLabs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
