{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading input data\n",
    "Load documents from the files, placed under given root folders (for training and testing separately).        \n",
    "Files wich belong to some specific category are placed into the same subfolder of the root. \n",
    "Name of such subfolder is treated as a name of category.    \n",
    "File which belongs to few categories should be copied into all correspondig subfolders (but will be loaded only once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load input data in 4 sec\n",
      "Dataset properties:\n",
      "Loaded 23837 documents: 12750 for training, 2251 for validation, 8836 for test\n",
      "Tokens in documents: maximum: 1330, minimum: 46, average: 170\n",
      "Categories: 40\n",
      "Documents for training in category : maximum: 2800, minimum: 93, avegare: 415\n",
      "Documents for testing  in category : maximum: 1501, minimum: 65, avegare: 276\n",
      "Distinct Label Set: 290\n",
      "Proportion of Distinct Label Set: 0.0193\n",
      "Label Cardinality: 1.1073\n",
      "Label Density: 0.0277\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from keras.models import load_model\n",
    "import statistics\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "homePath = str(Path.home()) + \"/MLClassificationData\"\n",
    "LabeledSentence = gensim.models.doc2vec.TaggedDocument\n",
    "# Dimension of word vectors\n",
    "n_dim = 100\n",
    "# Count of categories (labels).\n",
    "n_cats = 0\n",
    "# Count of words, which aren't found in w2v vocabulary.\n",
    "nf_words = 0\n",
    "# This id is included into the names of models, created by this notebook.\n",
    "modelId = 1\n",
    "# Path to tokenized corpus for training\n",
    "trainRoot = homePath + '/train/rtanews/target'\n",
    "# Path to tokenized corpus for testing\n",
    "testRoot = homePath + '/test/rtanews/target'\n",
    "# Path for saving best results, achieved in the time of training\n",
    "best_models_path = homePath + '/models/rtanews/tempModels/'\n",
    "# Part of the training corpus, used for runtime validation\n",
    "valPart = 0.15\n",
    "batch_size = 128\n",
    "random.seed(1)\n",
    "cnt = 0\n",
    "nCats = 0\n",
    "categories = dict()\n",
    "\n",
    "LabeledDocument = namedtuple('LabeledDocument', 'words labels qLabs')\n",
    "\n",
    "def getCategories(path):\n",
    "    cats = dict()\n",
    "    nCats = 0\n",
    "    os.chdir(path)\n",
    "    for f in glob.glob(\"*\"):\n",
    "        if os.path.isdir(f):\n",
    "            cats[f] = nCats\n",
    "            nCats += 1\n",
    "    return cats\n",
    "\n",
    "def prepareDocsData(path, cats):\n",
    "    files = dict()\n",
    "    fInCats = [0] * len(cats)\n",
    "    nFiles = 0\n",
    "    actFiles = 0\n",
    "    curCategory = 0\n",
    "    docs = []\n",
    "    os.chdir(path)\n",
    "    rootDir = os.getcwd()\n",
    "    for f in glob.glob(\"*\"):\n",
    "        curCategory = cats[f]\n",
    "        catPath = path + \"/\" + f\n",
    "        os.chdir(catPath)\n",
    "        for fc in glob.glob(\"*\"):\n",
    "            actFiles += 1\n",
    "            if fc not in files:\n",
    "                nFiles += 1\n",
    "                fPath = catPath + \"/\" + fc\n",
    "                docCont = ''\n",
    "                with open(fc, 'r', encoding='UTF-8') as tc:\n",
    "                    for line in tc:\n",
    "                        docCont += line.strip() + \" \"\n",
    "                tc.close()\n",
    "                words = docCont.strip().split() \n",
    "                labels = [0] * len(cats)\n",
    "                labels[curCategory] = 1\n",
    "                files[fc] = LabeledDocument(words, labels, [1])\n",
    "            else:\n",
    "                files[fc].labels[curCategory] = 1\n",
    "                files[fc].qLabs[0] += 1\n",
    "            fInCats[curCategory] += 1\n",
    "    for k, val in files.items():\n",
    "        docs.append(val)\n",
    "    return docs, fInCats\n",
    "\n",
    "def getLabelSets(docs):\n",
    "    labels = [x[1] for x in docs]\n",
    "    results = [labels[0]]\n",
    "    qLabs = 0\n",
    "    for i in range(len(labels)):        \n",
    "        if i%1000 == 0:\n",
    "            print (str(i), end='\\r')\n",
    "        qLabs += sum(labels[i])\n",
    "        count = 0\n",
    "        for j in range(len(results)):\n",
    "            for k in range(len(categories)):\n",
    "                if labels[i][k] != results[j][k]:\n",
    "                    count += 1\n",
    "                    break\n",
    "        if count == len(results):\n",
    "            results.append(labels[i])\n",
    "    return len(results), qLabs\n",
    "    \n",
    "def showTime(ds,de):\n",
    "    result = ''\n",
    "    seconds = (de-ds).total_seconds()\n",
    "    if seconds < 1:\n",
    "        return \"less than 1 sec\"\n",
    "    hh = int(seconds/(60*60));\n",
    "    if hh > 0:\n",
    "        result = \"%d h:\"%(hh);\n",
    "    seconds -= hh*60*60\n",
    "    mm = int(seconds/60);\n",
    "    if mm > 0:\n",
    "        result += \"%d min:\"%(mm)\n",
    "    ss = seconds - mm*60;\n",
    "    result += \"%d sec\"%(ss)\n",
    "    return result\n",
    "\n",
    "ds = datetime.datetime.now()\n",
    "categories = getCategories(trainRoot)\n",
    "#print (categories)\n",
    "trainAllDocs, fInCats1 = prepareDocsData(trainRoot, categories)\n",
    "trainAllDocs = random.sample(trainAllDocs, len(trainAllDocs))\n",
    "trainDocs = trainAllDocs[:int(len(trainAllDocs) * (1 - valPart))]\n",
    "valDocs = trainAllDocs[int(len(trainAllDocs) * (1 - valPart)):]\n",
    "testDocs, fInCats2 = prepareDocsData(testRoot, categories)\n",
    "testDocs = random.sample(testDocs, len(testDocs))\n",
    "maxDocLen = max(len(x.words) for x in trainAllDocs)\n",
    "minDocLen = min(len(x.words) for x in trainAllDocs)\n",
    "avrgDocLen = round(statistics.mean(len(x.words) for x in trainAllDocs), 2)\n",
    "de = datetime.datetime.now()\n",
    "\n",
    "print (\"Load input data in %s\"%(showTime(ds, de))) \n",
    "print (\"Dataset properties:\")\n",
    "dls, qLabs = getLabelSets(trainAllDocs)\n",
    "print ('Loaded %d documents: %d for training, %d for validation, %d for test' % (len(trainAllDocs) + len(testDocs), len(trainDocs), len(valDocs), len(testDocs)))\n",
    "print (\"Tokens in documents: maximum: %d, minimum: %d, average: %d\"%(maxDocLen, minDocLen, avrgDocLen))\n",
    "print (\"Categories: %d\"%(len(categories)))\n",
    "print (\"Documents for training in category : maximum: %d, minimum: %d, avegare: %d\"%(max(fInCats1), min(fInCats1), round(statistics.mean(fInCats1), 2)))\n",
    "print (\"Documents for testing  in category : maximum: %d, minimum: %d, avegare: %d\"%(max(fInCats2), min(fInCats2), round(statistics.mean(fInCats2), 2)))\n",
    "\n",
    "print (\"Distinct Label Set: %d\"%(dls))\n",
    "print (\"Proportion of Distinct Label Set: %.4f\"%(dls/len(trainAllDocs)))\n",
    "print (\"Label Cardinality: %.4f\"%(qLabs/len(trainAllDocs)))\n",
    "print (\"Label Density: %.4f\"%(qLabs/len(trainAllDocs)/len(categories)))\n",
    "\n",
    "del trainAllDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load W2V model\n",
    "Load Word2Vec model, saved previously in a text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load W2V model (wiki_ar.vec) in 1 min:14 sec\n",
      "Continue\n"
     ]
    }
   ],
   "source": [
    "# Name of file with word vectors\n",
    "#modelName = 'model-2018-Nov-05-173949.vec'\n",
    "modelName = 'wiki_ar.vec'\n",
    "ds = datetime.datetime.now()\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(homePath + '/w2v/vectors/' + modelName)\n",
    "de = datetime.datetime.now()\n",
    "print (\"Load W2V model (%s) in %s\"%(modelName, showTime(ds, de))) \n",
    "print (\"Continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for training and testing\n",
    "Input data is converted into array of sequences so that documents of different length are represented by sequences of numbers of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare train and validation data in 1 min:2 sec\n",
      "Prepare test data in 38 sec\n",
      "Unique words in all documents: 155089\n",
      "Words not found in the w2v vocabulary: 90398\n"
     ]
    }
   ],
   "source": [
    "nf_words = 0\n",
    "sdict = dict()\n",
    "nfw = []\n",
    "def getDocsArray(tokens, wvModel, dataType):\n",
    "    global n_dim\n",
    "    global tmpCount\n",
    "    global nf_words\n",
    "    global sdict\n",
    "    global nfw\n",
    "    tmpCount = tmpCount + 1\n",
    "    if tmpCount != 0 and tmpCount%1000 == 0:\n",
    "        print(dataType + \": prepare \", tmpCount, end=\"\\r\")\n",
    "        \n",
    "    vec = numpy.zeros(n_dim).reshape((1, n_dim))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        if word not in sdict:\n",
    "            sdict[word] = 1\n",
    "        else:\n",
    "            sdict[word] = sdict[word] + 1\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += wvModel[word].reshape((1, n_dim)) \n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            if sdict[word] == 1:\n",
    "                nf_words += 1\n",
    "                if nf_words < 10:\n",
    "                    nfw.append(word)\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "    \n",
    "ds = datetime.datetime.now()\n",
    "tmpCount = 0\n",
    "train_arrays = numpy.concatenate([getDocsArray(x.words, w2v, 'Train') for x in trainDocs])\n",
    "tmpCount = 0\n",
    "val_arrays = numpy.concatenate([getDocsArray(x.words, w2v, 'Validation') for x in valDocs])\n",
    "train_labels = numpy.concatenate([numpy.array(x.labels).reshape(1, len(categories)) for x in trainDocs])\n",
    "val_labels = numpy.concatenate([numpy.array(x.labels).reshape(1, len(categories)) for x in valDocs])\n",
    "tmpCount = 0\n",
    "de = datetime.datetime.now()\n",
    "print (\"Prepare train and validation data in %s\"%(showTime(ds, de)))\n",
    "\n",
    "ds = datetime.datetime.now()\n",
    "test_arrays = numpy.concatenate([getDocsArray(x.words, w2v, \"Test\") for x in testDocs])\n",
    "test_labels = numpy.concatenate([numpy.array(x.labels).reshape(1, len(categories)) for x in testDocs])\n",
    "de = datetime.datetime.now()\n",
    "print (\"Prepare test data in %s\"%(showTime(ds, de)))\n",
    "print (\"Unique words in all documents: %d\"%(len(sdict)))\n",
    "print (\"Words not found in the w2v vocabulary: %d\"%(nf_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create, train and test the model\n",
    "Classification model is created here as an linear stack of few regular densely-connected neural network's layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 38 sec\n",
      "8836/8836 [==============================] - 1s 84us/step\n",
      "Final model accuracy: 98.48%\n",
      "8836/8836 [==============================] - 1s 107us/step\n",
      "Last saved model accuracy: 98.49%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.metrics import categorical_accuracy\n",
    "import math\n",
    "\n",
    "def getModel(n_dim):\n",
    "    global batch_size\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim=n_dim))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(len(categories), activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model    \n",
    "\n",
    "model = getModel(n_dim)\n",
    "# Quantity of epochs (loops) in which model is trained\n",
    "eps = 30\n",
    "verbose = 0\n",
    "checkpoint = ModelCheckpoint(best_models_path + 'curModel%d.hdf5'%(modelId), monitor='val_acc', verbose=verbose, save_best_only=True, mode='auto')\n",
    "print(\"Start training...\", end='\\r')\n",
    "\n",
    "ds = datetime.datetime.now()\n",
    "model.fit(train_arrays, train_labels, epochs=eps, validation_data=(val_arrays, val_labels), batch_size=batch_size, verbose=0, callbacks=[checkpoint], shuffle=False)\n",
    "de = datetime.datetime.now()\n",
    "print (\"Model trained in %s\"%(showTime(ds, de)))\n",
    "\n",
    "# Evaluation of the final model\n",
    "scores = model.evaluate(test_arrays, test_labels, verbose=1)\n",
    "print(\"Final model accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Evaluation of the best model, saved in the training time\n",
    "model1 = load_model(best_models_path + 'curModel%d.hdf5'%(modelId))\n",
    "scores = model1.evaluate(test_arrays, test_labels, verbose=1)\n",
    "print(\"Last saved model accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model model1-2018-Nov-11-203644 saved in less than 1 sec\n"
     ]
    }
   ],
   "source": [
    "# Set 'False' if you want to use the best model, saved in the training time\n",
    "useFinalModel = True\n",
    "\n",
    "modelPath = homePath + \"/models/rtanews/models/\"\n",
    "modelName = \"model%d-%s\"%(modelId, datetime.datetime.now().strftime(\"%Y-%b-%d-%H%M%S\"))\n",
    "\n",
    "ds = datetime.datetime.now() \n",
    "if not useFinalModel:\n",
    "    del model\n",
    "    model = model1    \n",
    "model.save(modelPath + modelName)\n",
    "os.remove(best_models_path + 'curModel%d.hdf5'%(modelId))\n",
    "de = datetime.datetime.now()\n",
    "print (\"Model %s saved in %s\"%(modelName, showTime(ds,de)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check results visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 Tagged for: 19.Olympics_Ryu_d_Janero_2016 , 23.Sports_Other\n",
      "Predictions (with probability of more than one percent):\n",
      "\t      19.Olympics_Ryu_d_Janero_2016  89.37%\n",
      "\t                    23.Sports_Other  65.86%\n",
      "\n",
      "Document 101 Tagged for: 07.Crisis_Syrian conflict , 03.Weapons_and_military_equipment\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  57.67%\n",
      "\t                   25.Army_Aircraft  18.54%\n",
      "\t  03.Weapons_and_military_equipment  16.97%\n",
      "\t                         24.Rockets  16.18%\n",
      "\t      15.Opposition_Syrian conflict  5.40%\n",
      "\t                    22.Groups_armed  3.05%\n",
      "\t              37.Military_Maneuvers  2.49%\n",
      "\t          13.Technology_Information  1.87%\n",
      "\n",
      "Document 201 Tagged for: 17.Diseases\n",
      "Predictions (with probability of more than one percent):\n",
      "\t             36.General_information  12.34%\n",
      "\t                     05.Discoveries  8.05%\n",
      "\t                11.Research_Medical  7.82%\n",
      "\t                        17.Diseases  5.66%\n",
      "\t                           33.Drugs  5.57%\n",
      "\t              39.Economy_Indicators  4.16%\n",
      "\t      19.Olympics_Ryu_d_Janero_2016  3.91%\n",
      "\t                    22.Groups_armed  3.29%\n",
      "\t                16.Migration_Europe  1.92%\n",
      "\t                    23.Sports_Other  1.88%\n",
      "\t          13.Technology_Information  1.53%\n",
      "\t                          21.Crimes  1.29%\n",
      "\t                        30.Football  1.21%\n",
      "\t                        32.Refugees  1.17%\n",
      "\t                         24.Rockets  1.13%\n",
      "\n",
      "Document 301 Tagged for: 33.Drugs\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                           33.Drugs  56.48%\n",
      "\t                10.Election_America  4.70%\n",
      "\t      19.Olympics_Ryu_d_Janero_2016  3.20%\n",
      "\t                          21.Crimes  2.86%\n",
      "\t                    23.Sports_Other  2.25%\n",
      "\n",
      "Document 401 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.98%\n",
      "\n",
      "Document 501 Tagged for: 22.Groups_armed\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                    22.Groups_armed  40.62%\n",
      "\t                        32.Refugees  7.65%\n",
      "\t          07.Crisis_Syrian conflict  7.26%\n",
      "\t                    40.Attacs_Paris  4.90%\n",
      "\t                           33.Drugs  4.23%\n",
      "\t                16.Migration_Europe  3.76%\n",
      "\t                          21.Crimes  3.59%\n",
      "\n",
      "Document 601 Tagged for: 07.Crisis_Syrian conflict , 15.Opposition_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  59.74%\n",
      "\t                    22.Groups_armed  19.47%\n",
      "\t                        32.Refugees  10.47%\n",
      "\t      15.Opposition_Syrian conflict  8.31%\n",
      "\t                          21.Crimes  5.64%\n",
      "\t                      20.Explosions  2.69%\n",
      "\t              27.Archeology_History  1.18%\n",
      "\n",
      "Document 701 Tagged for: 25.Army_Aircraft\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                   25.Army_Aircraft  33.74%\n",
      "\t          07.Crisis_Syrian conflict  16.02%\n",
      "\t                31.Catastrophes_Air  10.96%\n",
      "\t                      20.Explosions  6.74%\n",
      "\t                    22.Groups_armed  6.73%\n",
      "\t  03.Weapons_and_military_equipment  2.85%\n",
      "\t                    40.Attacs_Paris  1.04%\n",
      "\t                        32.Refugees  1.00%\n",
      "\n",
      "Document 801 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  100.00%\n",
      "\n",
      "Document 901 Tagged for: 08.Crisis_Yemenis\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                  08.Crisis_Yemenis  93.54%\n",
      "\t                         24.Rockets  6.79%\n",
      "\t                    22.Groups_armed  2.26%\n",
      "\t  03.Weapons_and_military_equipment  1.67%\n",
      "\n",
      "Document 1001 Tagged for: 22.Groups_armed\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                    22.Groups_armed  78.67%\n",
      "\t                      20.Explosions  36.16%\n",
      "\n",
      "Document 1101 Tagged for: 25.Army_Aircraft , 35.Demonstrations\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              37.Military_Maneuvers  81.78%\n",
      "\t                   25.Army_Aircraft  39.07%\n",
      "\t  03.Weapons_and_military_equipment  10.79%\n",
      "\t                         24.Rockets  4.55%\n",
      "\t          07.Crisis_Syrian conflict  1.22%\n",
      "\n",
      "Document 1201 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  75.28%\n",
      "\t               26.Economy_Sanctions  16.40%\n",
      "\t                    22.Groups_armed  8.75%\n",
      "\t  01.Agreement_Iran_Nuclear_weapons  2.40%\n",
      "\t  03.Weapons_and_military_equipment  2.15%\n",
      "\t      15.Opposition_Syrian conflict  1.58%\n",
      "\n",
      "Document 1301 Tagged for: 20.Explosions\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                      20.Explosions  93.45%\n",
      "\t                    22.Groups_armed  19.27%\n",
      "\n",
      "Document 1401 Tagged for: 39.Economy_Indicators\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              39.Economy_Indicators  86.65%\n",
      "\t               26.Economy_Sanctions  8.70%\n",
      "\t                06.Crisis_Ukrainian  4.95%\n",
      "\t                     04.Oil_markets  4.16%\n",
      "\t              02.Referendum_Britain  3.40%\n",
      "\t                  12.StockExchanges  1.12%\n",
      "\n",
      "Document 1501 Tagged for: 07.Crisis_Syrian conflict , 22.Groups_armed\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  93.26%\n",
      "\t      15.Opposition_Syrian conflict  11.11%\n",
      "\t                    22.Groups_armed  3.02%\n",
      "\n",
      "Document 1601 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  98.15%\n",
      "\t               26.Economy_Sanctions  1.20%\n",
      "\t      15.Opposition_Syrian conflict  1.11%\n",
      "\t                    22.Groups_armed  1.06%\n",
      "\n",
      "Document 1701 Tagged for: 21.Crimes , 34.Famous\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                          34.Famous  97.15%\n",
      "\t                          21.Crimes  4.00%\n",
      "\t                           38.Music  1.44%\n",
      "\t             36.General_information  1.40%\n",
      "\n",
      "Document 1801 Tagged for: 03.Weapons_and_military_equipment\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                06.Crisis_Ukrainian  90.85%\n",
      "\t          07.Crisis_Syrian conflict  13.78%\n",
      "\t  03.Weapons_and_military_equipment  6.51%\n",
      "\t               26.Economy_Sanctions  2.31%\n",
      "\t                        32.Refugees  1.36%\n",
      "\n",
      "Document 1901 Tagged for: 13.Technology_Information\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          13.Technology_Information  99.71%\n",
      "\t             36.General_information  2.65%\n",
      "\n",
      "Document 2001 Tagged for: 24.Rockets\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                         24.Rockets  98.53%\n",
      "\t                           29.Space  2.02%\n",
      "\t          13.Technology_Information  1.59%\n",
      "\t  03.Weapons_and_military_equipment  1.52%\n",
      "\n",
      "Document 2101 Tagged for: 11.Research_Medical\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                11.Research_Medical  85.89%\n",
      "\t             36.General_information  18.09%\n",
      "\t                        17.Diseases  15.15%\n",
      "\n",
      "Document 2201 Tagged for: 05.Discoveries\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                     05.Discoveries  65.37%\n",
      "\t                           29.Space  24.36%\n",
      "\t          13.Technology_Information  1.53%\n",
      "\n",
      "Document 2301 Tagged for: 04.Oil_markets\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                     04.Oil_markets  94.58%\n",
      "\t                          34.Famous  9.99%\n",
      "\n",
      "Document 2401 Tagged for: 26.Economy_Sanctions\n",
      "Predictions (with probability of more than one percent):\n",
      "\t               26.Economy_Sanctions  89.58%\n",
      "\t          07.Crisis_Syrian conflict  10.19%\n",
      "\t  01.Agreement_Iran_Nuclear_weapons  9.82%\n",
      "\t                         24.Rockets  6.84%\n",
      "\t  03.Weapons_and_military_equipment  4.92%\n",
      "\t                06.Crisis_Ukrainian  2.40%\n",
      "\t              39.Economy_Indicators  1.37%\n",
      "\t                     04.Oil_markets  1.05%\n",
      "\n",
      "Document 2501 Tagged for: 24.Rockets\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              37.Military_Maneuvers  67.41%\n",
      "\t                         24.Rockets  66.77%\n",
      "\t  03.Weapons_and_military_equipment  10.84%\n",
      "\t          07.Crisis_Syrian conflict  5.63%\n",
      "\n",
      "Document 2601 Tagged for: 15.Opposition_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  94.16%\n",
      "\t      15.Opposition_Syrian conflict  12.50%\n",
      "\t                    22.Groups_armed  1.83%\n",
      "\n",
      "Document 2701 Tagged for: 33.Drugs\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                           33.Drugs  98.24%\n",
      "\t                11.Research_Medical  2.09%\n",
      "\t                          21.Crimes  2.02%\n",
      "\n",
      "Document 2801 Tagged for: 26.Economy_Sanctions , 04.Oil_markets\n",
      "Predictions (with probability of more than one percent):\n",
      "\t               26.Economy_Sanctions  80.62%\n",
      "\t              39.Economy_Indicators  29.57%\n",
      "\t                     04.Oil_markets  17.51%\n",
      "\t  03.Weapons_and_military_equipment  5.42%\n",
      "\t                06.Crisis_Ukrainian  4.25%\n",
      "\t                         24.Rockets  1.95%\n",
      "\t  01.Agreement_Iran_Nuclear_weapons  1.68%\n",
      "\n",
      "Document 2901 Tagged for: 29.Space\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                           29.Space  99.62%\n",
      "\t                     05.Discoveries  1.20%\n",
      "\n",
      "Document 3001 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.99%\n",
      "\n",
      "Document 3101 Tagged for: 25.Army_Aircraft\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                   25.Army_Aircraft  80.29%\n",
      "\t          07.Crisis_Syrian conflict  42.71%\n",
      "\t  03.Weapons_and_military_equipment  1.60%\n",
      "\t              37.Military_Maneuvers  1.02%\n",
      "\n",
      "Document 3201 Tagged for: 25.Army_Aircraft , 37.Military_Maneuvers , 03.Weapons_and_military_equipment\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              37.Military_Maneuvers  89.65%\n",
      "\t                         24.Rockets  17.95%\n",
      "\t                   25.Army_Aircraft  8.61%\n",
      "\t  03.Weapons_and_military_equipment  6.37%\n",
      "\t          07.Crisis_Syrian conflict  1.58%\n",
      "\n",
      "Document 3301 Tagged for: 11.Research_Medical\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                11.Research_Medical  46.23%\n",
      "\t                        17.Diseases  43.35%\n",
      "\t                     05.Discoveries  21.82%\n",
      "\t             36.General_information  12.75%\n",
      "\n",
      "Document 3401 Tagged for: 25.Army_Aircraft\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              37.Military_Maneuvers  52.29%\n",
      "\t                   25.Army_Aircraft  38.35%\n",
      "\t  03.Weapons_and_military_equipment  21.32%\n",
      "\t                         24.Rockets  9.63%\n",
      "\n",
      "Document 3501 Tagged for: 16.Migration_Europe , 35.Demonstrations\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        32.Refugees  76.95%\n",
      "\t                16.Migration_Europe  43.04%\n",
      "\t                  35.Demonstrations  17.98%\n",
      "\t              02.Referendum_Britain  1.96%\n",
      "\t                06.Crisis_Ukrainian  1.82%\n",
      "\t          07.Crisis_Syrian conflict  1.00%\n",
      "\n",
      "Document 3601 Tagged for: 09.Recognition_State_Palestine\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        32.Refugees  53.64%\n",
      "\t                          34.Famous  11.66%\n",
      "\t          07.Crisis_Syrian conflict  9.71%\n",
      "\t                          21.Crimes  6.52%\n",
      "\t                16.Migration_Europe  5.97%\n",
      "\t                   14.Tourism_World  3.32%\n",
      "\t                    22.Groups_armed  2.90%\n",
      "\t     09.Recognition_State_Palestine  2.13%\n",
      "\t              27.Archeology_History  1.36%\n",
      "\t                    40.Attacs_Paris  1.17%\n",
      "\n",
      "Document 3701 Tagged for: 08.Crisis_Yemenis\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                  08.Crisis_Yemenis  36.42%\n",
      "\t                    22.Groups_armed  23.61%\n",
      "\t                        32.Refugees  4.61%\n",
      "\t          07.Crisis_Syrian conflict  3.88%\n",
      "\t                16.Migration_Europe  2.15%\n",
      "\t                06.Crisis_Ukrainian  2.11%\n",
      "\t              39.Economy_Indicators  2.08%\n",
      "\t                          21.Crimes  1.17%\n",
      "\n",
      "Document 3801 Tagged for: 07.Crisis_Syrian conflict , 15.Opposition_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  64.14%\n",
      "\t      15.Opposition_Syrian conflict  56.81%\n",
      "\t                    22.Groups_armed  24.72%\n",
      "\n",
      "Document 3901 Tagged for: 06.Crisis_Ukrainian , 21.Crimes\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                          21.Crimes  52.66%\n",
      "\t                06.Crisis_Ukrainian  15.50%\n",
      "\t          07.Crisis_Syrian conflict  6.50%\n",
      "\t                    22.Groups_armed  5.46%\n",
      "\t                      20.Explosions  1.11%\n",
      "\n",
      "Document 4001 Tagged for: 20.Explosions\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                    22.Groups_armed  61.43%\n",
      "\t                      20.Explosions  18.97%\n",
      "\t                   25.Army_Aircraft  11.79%\n",
      "\t          07.Crisis_Syrian conflict  4.53%\n",
      "\t                31.Catastrophes_Air  3.44%\n",
      "\t                        32.Refugees  1.48%\n",
      "\n",
      "Document 4101 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.90%\n",
      "\n",
      "Document 4201 Tagged for: 14.Tourism_World\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                   14.Tourism_World  93.60%\n",
      "\t                    22.Groups_armed  5.57%\n",
      "\t                        32.Refugees  2.30%\n",
      "\t                16.Migration_Europe  2.24%\n",
      "\t                31.Catastrophes_Air  1.78%\n",
      "\t                    40.Attacs_Paris  1.61%\n",
      "\n",
      "Document 4301 Tagged for: 16.Migration_Europe , 32.Refugees\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        32.Refugees  85.99%\n",
      "\t                16.Migration_Europe  49.79%\n",
      "\t                  35.Demonstrations  22.76%\n",
      "\t          07.Crisis_Syrian conflict  2.98%\n",
      "\n",
      "Document 4401 Tagged for: 11.Research_Medical , 38.Music\n",
      "Predictions (with probability of more than one percent):\n",
      "\t             36.General_information  72.37%\n",
      "\t                11.Research_Medical  29.06%\n",
      "\t          13.Technology_Information  13.82%\n",
      "\t                     05.Discoveries  4.42%\n",
      "\t                        17.Diseases  3.90%\n",
      "\n",
      "Document 4501 Tagged for: 08.Crisis_Yemenis , 22.Groups_armed\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                  08.Crisis_Yemenis  93.94%\n",
      "\t                         24.Rockets  2.68%\n",
      "\n",
      "Document 4601 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.91%\n",
      "\n",
      "Document 4701 Tagged for: 40.Attacs_Paris\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                    22.Groups_armed  63.00%\n",
      "\t                    40.Attacs_Paris  44.50%\n",
      "\t                      20.Explosions  11.87%\n",
      "\t          07.Crisis_Syrian conflict  7.86%\n",
      "\t                          21.Crimes  5.39%\n",
      "\n",
      "Document 4801 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  91.92%\n",
      "\t                    23.Sports_Other  22.14%\n",
      "\t      19.Olympics_Ryu_d_Janero_2016  1.27%\n",
      "\n",
      "Document 4901 Tagged for: 03.Weapons_and_military_equipment\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  14.10%\n",
      "\t          13.Technology_Information  9.19%\n",
      "\t  03.Weapons_and_military_equipment  7.05%\n",
      "\t                           33.Drugs  3.76%\n",
      "\t                06.Crisis_Ukrainian  2.98%\n",
      "\t                    22.Groups_armed  2.05%\n",
      "\t                        32.Refugees  1.65%\n",
      "\t                          21.Crimes  1.35%\n",
      "\t              39.Economy_Indicators  1.35%\n",
      "\n",
      "Document 5001 Tagged for: 25.Army_Aircraft\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                   25.Army_Aircraft  80.64%\n",
      "\t          07.Crisis_Syrian conflict  9.11%\n",
      "\t  03.Weapons_and_military_equipment  4.57%\n",
      "\t              37.Military_Maneuvers  3.58%\n",
      "\t                31.Catastrophes_Air  3.15%\n",
      "\t                         24.Rockets  1.56%\n",
      "\t                06.Crisis_Ukrainian  1.14%\n",
      "\n",
      "Document 5101 Tagged for: 29.Space\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                           29.Space  56.51%\n",
      "\t          13.Technology_Information  40.58%\n",
      "\t             36.General_information  22.51%\n",
      "\t                         24.Rockets  15.85%\n",
      "\t                     05.Discoveries  9.85%\n",
      "\n",
      "Document 5201 Tagged for: 06.Crisis_Ukrainian , 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  74.89%\n",
      "\t                06.Crisis_Ukrainian  3.50%\n",
      "\t          13.Technology_Information  2.04%\n",
      "\t                   14.Tourism_World  1.46%\n",
      "\t                   25.Army_Aircraft  1.16%\n",
      "\n",
      "Document 5301 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  67.05%\n",
      "\t      15.Opposition_Syrian conflict  44.69%\n",
      "\t                    22.Groups_armed  14.92%\n",
      "\t                        32.Refugees  2.14%\n",
      "\n",
      "Document 5401 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  91.31%\n",
      "\t      15.Opposition_Syrian conflict  12.54%\n",
      "\t                    22.Groups_armed  8.00%\n",
      "\t                        32.Refugees  1.28%\n",
      "\n",
      "Document 5501 Tagged for: 12.StockExchanges\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                  12.StockExchanges  61.11%\n",
      "\t              39.Economy_Indicators  31.24%\n",
      "\t          13.Technology_Information  19.65%\n",
      "\n",
      "Document 5601 Tagged for: 22.Groups_armed , 40.Attacs_Paris\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                    40.Attacs_Paris  66.91%\n",
      "\t                    22.Groups_armed  62.22%\n",
      "\t                          21.Crimes  3.07%\n",
      "\t                        30.Football  2.84%\n",
      "\t                      20.Explosions  2.62%\n",
      "\t                    23.Sports_Other  2.01%\n",
      "\n",
      "Document 5701 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.83%\n",
      "\n",
      "Document 5801 Tagged for: 37.Military_Maneuvers , 03.Weapons_and_military_equipment\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              37.Military_Maneuvers  88.37%\n",
      "\t  03.Weapons_and_military_equipment  12.15%\n",
      "\t          07.Crisis_Syrian conflict  11.80%\n",
      "\t                         24.Rockets  4.25%\n",
      "\t                   25.Army_Aircraft  3.57%\n",
      "\t                    22.Groups_armed  1.15%\n",
      "\n",
      "Document 5901 Tagged for: 23.Sports_Other\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                    23.Sports_Other  99.99%\n",
      "\n",
      "Document 6001 Tagged for: 38.Music\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                           38.Music  89.76%\n",
      "\t                          34.Famous  62.84%\n",
      "\t             36.General_information  2.42%\n",
      "\t          13.Technology_Information  2.01%\n",
      "\n",
      "Document 6101 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.33%\n",
      "\t                    23.Sports_Other  1.79%\n",
      "\n",
      "Document 6201 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  83.45%\n",
      "\t                    22.Groups_armed  10.98%\n",
      "\t      15.Opposition_Syrian conflict  8.76%\n",
      "\t                   25.Army_Aircraft  4.13%\n",
      "\t                06.Crisis_Ukrainian  3.81%\n",
      "\t  03.Weapons_and_military_equipment  2.04%\n",
      "\t                  08.Crisis_Yemenis  1.74%\n",
      "\t                        32.Refugees  1.64%\n",
      "\t                    40.Attacs_Paris  1.23%\n",
      "\n",
      "Document 6301 Tagged for: 06.Crisis_Ukrainian , 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  92.95%\n",
      "\t                06.Crisis_Ukrainian  9.86%\n",
      "\t                        32.Refugees  1.18%\n",
      "\n",
      "Document 6401 Tagged for: 03.Weapons_and_military_equipment\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  57.54%\n",
      "\t                   25.Army_Aircraft  45.81%\n",
      "\t                31.Catastrophes_Air  3.00%\n",
      "\t                         24.Rockets  1.31%\n",
      "\t  03.Weapons_and_military_equipment  1.07%\n",
      "\n",
      "Document 6501 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.98%\n",
      "\n",
      "Document 6601 Tagged for: 32.Refugees , 34.Famous\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                          34.Famous  98.07%\n",
      "\t                        32.Refugees  1.05%\n",
      "\n",
      "Document 6701 Tagged for: 39.Economy_Indicators\n",
      "Predictions (with probability of more than one percent):\n",
      "\t              39.Economy_Indicators  30.19%\n",
      "\t                           33.Drugs  9.27%\n",
      "\t               26.Economy_Sanctions  8.82%\n",
      "\t          13.Technology_Information  7.06%\n",
      "\t                    23.Sports_Other  4.46%\n",
      "\t      19.Olympics_Ryu_d_Janero_2016  3.90%\n",
      "\t  03.Weapons_and_military_equipment  3.72%\n",
      "\t             36.General_information  2.41%\n",
      "\t                     04.Oil_markets  1.80%\n",
      "\t                         24.Rockets  1.71%\n",
      "\t                10.Election_America  1.61%\n",
      "\t                06.Crisis_Ukrainian  1.09%\n",
      "\n",
      "Document 6801 Tagged for: 19.Olympics_Ryu_d_Janero_2016 , 23.Sports_Other , 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t      19.Olympics_Ryu_d_Janero_2016  91.38%\n",
      "\t                    23.Sports_Other  40.75%\n",
      "\t                        30.Football  4.99%\n",
      "\n",
      "Document 6901 Tagged for: 32.Refugees\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        32.Refugees  92.44%\n",
      "\t                16.Migration_Europe  40.51%\n",
      "\t          07.Crisis_Syrian conflict  10.90%\n",
      "\n",
      "Document 7001 Tagged for: 10.Election_America\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                10.Election_America  95.25%\n",
      "\t                    22.Groups_armed  12.29%\n",
      "\t          07.Crisis_Syrian conflict  4.25%\n",
      "\n",
      "Document 7101 Tagged for: 04.Oil_markets\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                     04.Oil_markets  99.85%\n",
      "\n",
      "Document 7201 Tagged for: 36.General_information\n",
      "Predictions (with probability of more than one percent):\n",
      "\t             36.General_information  73.14%\n",
      "\t                           29.Space  56.19%\n",
      "\t                     05.Discoveries  14.15%\n",
      "\t                11.Research_Medical  1.24%\n",
      "\n",
      "Document 7301 Tagged for: 29.Space\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                           29.Space  99.50%\n",
      "\t                     05.Discoveries  2.21%\n",
      "\n",
      "Document 7401 Tagged for: 15.Opposition_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  75.81%\n",
      "\t      15.Opposition_Syrian conflict  35.42%\n",
      "\t                    22.Groups_armed  19.57%\n",
      "\n",
      "Document 7501 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t                        30.Football  99.77%\n",
      "\t                    23.Sports_Other  1.31%\n",
      "\n",
      "Document 7601 Tagged for: 34.Famous\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                          34.Famous  88.53%\n",
      "\t             36.General_information  12.81%\n",
      "\t                           38.Music  6.06%\n",
      "\n",
      "Document 7701 Tagged for: 13.Technology_Information\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          13.Technology_Information  98.69%\n",
      "\t                     05.Discoveries  3.32%\n",
      "\t             36.General_information  2.96%\n",
      "\n",
      "Document 7801 Tagged for: 05.Discoveries , 13.Technology_Information\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          13.Technology_Information  80.08%\n",
      "\t             36.General_information  33.58%\n",
      "\t                     05.Discoveries  7.61%\n",
      "\t                11.Research_Medical  3.00%\n",
      "\t                          21.Crimes  1.38%\n",
      "\t                    22.Groups_armed  1.20%\n",
      "\n",
      "Document 7901 Tagged for: 04.Oil_markets\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                     04.Oil_markets  39.87%\n",
      "\t      15.Opposition_Syrian conflict  7.75%\n",
      "\t                    22.Groups_armed  6.86%\n",
      "\n",
      "Document 8001 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.22%\n",
      "\t                    23.Sports_Other  3.18%\n",
      "\n",
      "Document 8101 Tagged for: 30.Football\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                        30.Football  99.60%\n",
      "\t                    23.Sports_Other  2.33%\n",
      "\n",
      "Document 8201 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  65.35%\n",
      "\t      15.Opposition_Syrian conflict  40.39%\n",
      "\t                    22.Groups_armed  13.73%\n",
      "\n",
      "Document 8301 Tagged for: 08.Crisis_Yemenis\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                  08.Crisis_Yemenis  94.47%\n",
      "\t                    22.Groups_armed  1.12%\n",
      "\n",
      "Document 8401 Tagged for: 20.Explosions\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                      20.Explosions  45.46%\n",
      "\t                    22.Groups_armed  20.13%\n",
      "\t                    40.Attacs_Paris  9.59%\n",
      "\t          07.Crisis_Syrian conflict  3.17%\n",
      "\t                31.Catastrophes_Air  1.91%\n",
      "\t                        32.Refugees  1.85%\n",
      "\t         28.Operation_Freeing_Mosul  1.22%\n",
      "\n",
      "Document 8501 Tagged for: 07.Crisis_Syrian conflict\n",
      "Predictions (with probability of more than one percent):\n",
      "\t          07.Crisis_Syrian conflict  91.77%\n",
      "\t      15.Opposition_Syrian conflict  5.57%\n",
      "\t                    22.Groups_armed  3.04%\n",
      "\t                        32.Refugees  2.15%\n",
      "\n",
      "Document 8601 Tagged for: 32.Refugees\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                          21.Crimes  43.98%\n",
      "\t                        32.Refugees  27.53%\n",
      "\t                16.Migration_Europe  6.23%\n",
      "\t          07.Crisis_Syrian conflict  4.24%\n",
      "\t                          34.Famous  3.91%\n",
      "\t                    40.Attacs_Paris  3.12%\n",
      "\t                    22.Groups_armed  2.79%\n",
      "\t                        17.Diseases  2.25%\n",
      "\t             36.General_information  1.25%\n",
      "\n",
      "Document 8701 Tagged for: 20.Explosions\n",
      "Predictions (with probability of more than one percent):\n",
      "\t                      20.Explosions  77.31%\n",
      "\t                    22.Groups_armed  25.00%\n",
      "\t                31.Catastrophes_Air  13.19%\n",
      "\t          07.Crisis_Syrian conflict  1.19%\n",
      "\n",
      "Document 8801 Tagged for: 28.Operation_Freeing_Mosul\n",
      "Predictions (with probability of more than one percent):\n",
      "\t         28.Operation_Freeing_Mosul  58.61%\n",
      "\t                    22.Groups_armed  42.05%\n",
      "\t          07.Crisis_Syrian conflict  10.22%\n",
      "\t                      20.Explosions  6.90%\n",
      "\t      15.Opposition_Syrian conflict  1.19%\n",
      "\t                        32.Refugees  1.14%\n"
     ]
    }
   ],
   "source": [
    "cNames = [''] * len(categories)\n",
    "step = 100\n",
    "\n",
    "def getPrediction(entry):\n",
    "    return entry[1]\n",
    "\n",
    "for k,v in categories.items():\n",
    "    cNames[v] = k\n",
    "for id in range(0, len(test_arrays)-1, step):\n",
    "    if id > 0:\n",
    "        print()\n",
    "    taggedFor = \"Tagged for: \"\n",
    "    cTags = 0\n",
    "    for i in range(len(categories)):\n",
    "        if test_labels[id][i] == 1:\n",
    "            if cTags != 0:\n",
    "                taggedFor += \" , \"\n",
    "            cTags += 1        \n",
    "            taggedFor += cNames[i]        \n",
    "    res = model.predict(test_arrays[id].reshape(1, n_dim))\n",
    "    list = [(0,0) for i in range(len(categories))]\n",
    "    for i in range(len(categories)):\n",
    "        list[i] = (i, res[0][i])\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    print (\"Document %d %s\"%(id+1, taggedFor))\n",
    "    print (\"Predictions (with probability of more than one percent):\")\n",
    "    for i in range(len(categories)):\n",
    "        if list[i][1] >= 0.01:\n",
    "            print (\"\\t%35s  %.2f%%\" % (cNames[list[i][0]], list[i][1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate different metrics\n",
    "All metrics are calculated using the following indicator function: some label is treated as predicted, if its probability isn't less than some threshold.    \n",
    "We set 50% as a value of this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset containing 8836 documents predicted in less than 1 sec\n",
      "\n",
      "Exact Match Ratio:  60.19%\n",
      "Accuracy:  69.23%\n",
      "Precision:  71.37%\n",
      "Recall:  76.64%\n",
      "F1-Measure:  72.38%\n",
      "Hamming Loss:  1.52%\n",
      "Macro-Averaged Precision:  61.20%\n",
      "Macro-Averaged Recall:  80.27%\n",
      "Macro-Averaged F1-Measure:  67.52%\n",
      "Micro-Averaged Precision:  65.52%\n",
      "Micro-Averaged Recall:  82.28%\n",
      "Micro-Averaged F1-Measure:  72.95%\n",
      "One Error: 19.39%\n",
      "Coverage: 0.76\n",
      "Ranking Loss: 0.61\n"
     ]
    }
   ],
   "source": [
    "rankThreshold = 0.5\n",
    "\n",
    "def rankIndicator(labels, predictions, index):\n",
    "    global rankThreshold\n",
    "    return (labels[index] == 1), (predictions[index] >= rankThreshold)\n",
    "    \n",
    "def getPrediction(entry):\n",
    "    return entry[1]\n",
    "\n",
    "ds = datetime.datetime.now()\n",
    "res = model.predict(test_arrays)\n",
    "de = datetime.datetime.now()\n",
    "print (\"Dataset containing %d documents predicted in %s\\n\"%(len(test_arrays), showTime(ds, de)))\n",
    "\n",
    "#Exact Match Ratio\n",
    "wrongPreds = 0\n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if (actual and not predicted) or (predicted and not actual):\n",
    "            wrongPreds += 1\n",
    "            break;\n",
    "print (\"Exact Match Ratio:  %.2f%%\" % ((len(test_labels) - wrongPreds)/len(test_labels) * 100))\n",
    "\n",
    "#Accuracy\n",
    "accuracy = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "        if predicted and not actual:\n",
    "            tfp += 1\n",
    "    accuracy += tp / (labels + tfp)\n",
    "print (\"Accuracy:  %.2f%%\" % (accuracy / len(test_labels) * 100))  \n",
    "\n",
    "#Precision\n",
    "precision = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "    precision += tp / labels\n",
    "print (\"Precision:  %.2f%%\" % (precision / len(test_labels) * 100))  \n",
    "\n",
    "#Recall\n",
    "recall = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "        if predicted:\n",
    "            tfp += 1\n",
    "    if tfp > 0:\n",
    "        recall += tp / tfp\n",
    "print (\"Recall:  %.2f%%\" % (recall / len(test_labels) * 100))  \n",
    "\n",
    "#F1-Measure\n",
    "f1 = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    tp = 0\n",
    "    tfp = 0\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if actual and predicted:\n",
    "            tp += 1\n",
    "        if predicted:\n",
    "            tfp += 1\n",
    "    f1 += 2 * tp / (tfp + labels)\n",
    "print (\"F1-Measure:  %.2f%%\" % (f1 / len(test_labels) * 100))\n",
    "\n",
    "#Hamming Loss\n",
    "hl = 0.\n",
    "for i in range(len(test_labels)):\n",
    "    labels = sum(test_labels[i])\n",
    "    for j in range(len(categories)):\n",
    "        actual, predicted = rankIndicator(test_labels[i], res[i], j)\n",
    "        if (actual and not predicted) or (predicted and not actual):\n",
    "            hl += 1\n",
    "print (\"Hamming Loss:  %.2f%%\" % (hl * 100 / (len(test_labels) * len(categories)))) \n",
    "\n",
    "#Macro-Averaged Precision\n",
    "precision = 0\n",
    "for i in range(len(categories)):\n",
    "    tp = 0\n",
    "    tact = 0\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i) \n",
    "        if not actual:\n",
    "            continue\n",
    "        tact += 1\n",
    "        if predicted:\n",
    "            tp += 1\n",
    "    precision += tp / tact\n",
    "print (\"Macro-Averaged Precision:  %.2f%%\" % (precision / len(categories) * 100))  \n",
    "\n",
    "#Macro-Averaged Recall\n",
    "recall = 0\n",
    "for i in range(len(categories)):\n",
    "    tp = 0\n",
    "    tact = 0\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "    recall += tp / tact\n",
    "print (\"Macro-Averaged Recall:  %.2f%%\" % (recall / len(categories) * 100))  \n",
    "\n",
    "#Macro-Averaged F1-Measure\n",
    "f1 = 0\n",
    "for i in range(len(categories)):\n",
    "    tp = 0\n",
    "    tact = 0\n",
    "    labs = 0\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if actual:\n",
    "            labs += 1\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "    f1 += 2 * tp / (tact + labs)\n",
    "print (\"Macro-Averaged F1-Measure:  %.2f%%\" % (f1 / len(categories) * 100))  \n",
    "\n",
    "#Micro-Averaged Precision\n",
    "precision = 0\n",
    "tp = 0\n",
    "tact = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i) \n",
    "        if not actual:\n",
    "            continue\n",
    "        tact += 1\n",
    "        if predicted:\n",
    "            tp += 1\n",
    "precision += tp / tact\n",
    "print (\"Micro-Averaged Precision:  %.2f%%\" % (precision * 100))  \n",
    "\n",
    "#Micro-Averaged Recall\n",
    "recall = 0\n",
    "tp = 0\n",
    "tact = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "recall += tp / tact\n",
    "print (\"Micro-Averaged Recall:  %.2f%%\" % (recall * 100))  \n",
    "\n",
    "#Micro-Averaged F1-Measure\n",
    "f1 = 0\n",
    "tp = 0\n",
    "tact = 0\n",
    "labs = 0\n",
    "for i in range(len(categories)):\n",
    "    for j in range(len(test_labels)):\n",
    "        actual, predicted = rankIndicator(test_labels[j], res[j], i)\n",
    "        if actual:\n",
    "            labs += 1\n",
    "        if predicted:\n",
    "            tact += 1\n",
    "            if actual:\n",
    "                tp += 1\n",
    "f1 += 2 * tp / (tact + labs)\n",
    "print (\"Micro-Averaged F1-Measure:  %.2f%%\" % (f1 * 100))  \n",
    "\n",
    "#One error\n",
    "o_err = 0\n",
    "for i in range(len(test_labels)):\n",
    "    list = [(0,0) for i in range(len(categories))]\n",
    "    for j in range(len(categories)):\n",
    "        list[j] = (test_labels[i][j], res[i][j])\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    if list[0][0] == 0:\n",
    "        o_err += 1\n",
    "print (\"One Error: %.2f%%\" % (o_err / len(test_labels) * 100))\n",
    "\n",
    "#Coverage\n",
    "stepsDown = 0\n",
    "for i in range(len(test_labels)):\n",
    "    bound = sum(test_labels[i]) - 1\n",
    "    list = [(0,0,0) for i in range(len(categories))]\n",
    "    for j in range(len(categories)):\n",
    "        list[j] = (test_labels[i][j], res[i][j], j)\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    eSteps = 0\n",
    "    for j in range(len(categories)):\n",
    "        if test_labels[i][j] == 0:\n",
    "            continue\n",
    "        for k in range(len(list)):\n",
    "            if list[k][2] == j:\n",
    "                eSteps = max(eSteps, k)\n",
    "    stepsDown += max(0, eSteps - bound)\n",
    "print (\"Coverage: %.2f\" % (stepsDown / len(test_labels))) \n",
    "\n",
    "#Ranking Loss\n",
    "rl = 0\n",
    "for i in range(len(test_labels)):\n",
    "    mult = sum(test_labels[i])\n",
    "    list = [(0,0) for i in range(len(categories))]\n",
    "    wrongOrder = 0\n",
    "    for j in range(len(categories)):\n",
    "        list[j] = (test_labels[i][j], res[i][j])\n",
    "    list.sort(key=getPrediction, reverse=True)\n",
    "    for j in range(len(list)):\n",
    "        if list[j][0] == 1:\n",
    "            mult -= 1\n",
    "            if mult == 0:\n",
    "                break\n",
    "            continue\n",
    "        wrongOrder += mult\n",
    "    rl += wrongOrder / sum(test_labels[i])\n",
    "print (\"Ranking Loss: %.2f\" % (rl / len(test_labels)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
