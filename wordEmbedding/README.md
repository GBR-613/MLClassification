## Word Embedding
Word embedding is the name of technique in natural language processing, where words are mapped 
to vectors of real numbers, so that similar words or words used in a similar context are close to each other in the vector space. 
As an unsupervised learning technique, it can be trained on any corpus without the need for any human annotation, and 
provide a starting point for training any neural network.
