{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input data\n",
    "It is assumed here, that all related files are placed in the folder ~/MLClassificationData/w2v:    \n",
    "- source files with tokenized content - under subfolder _target_, \n",
    "- models, saved in binary format - under subfolder _models_,\n",
    "- models, saved in text format = under subfolder _vectors_.    \n",
    "\n",
    "Script works recursively and merge contents of all files, found in subfolder _target_ and all its subfolders.    \n",
    "Models are saved with unique names, containing the date and time of their creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3879950 lines from file /home/user/MLClassificationData/w2v/target/wiki_ar.txt\n",
      "At all: got 3879950 lines in 8 min:20 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from nltk.stem.util import suffix_replace\n",
    "\n",
    "homePath = str(Path.home()) + \"/MLClassificationData\"\n",
    "dataPath = homePath + \"/w2v/target\"\n",
    "modelPath = homePath + \"/w2v/models/\"\n",
    "vecPath = homePath + \"/w2v/vectors/\"\n",
    "n_dim = 100\n",
    "sentences = []\n",
    "cuDir = os.getcwd()\n",
    "\n",
    "class ArabicNormalizer(object):\n",
    "    __vocalization = re.compile(r'[\\u064b-\\u064c-\\u064d-\\u064e-\\u064f-\\u0650-\\u0651-\\u0652]')\n",
    "    __kasheeda = re.compile(r'[\\u0640]') # tatweel/kasheeda\n",
    "    __arabic_punctuation_marks = re.compile(r'[\\u060C-\\u061B-\\u061F]')\n",
    "    __last_hamzat = ('\\u0623', '\\u0625', '\\u0622', '\\u0624', '\\u0626') \n",
    "    __initial_hamzat = re.compile(r'^[\\u0622\\u0623\\u0625]')\n",
    "    __waw_hamza = re.compile(r'[\\u0624]') \n",
    "    __yeh_hamza = re.compile(r'[\\u0626]')\n",
    "    __alefat = re.compile(r'[\\u0623\\u0622\\u0625]')\n",
    "\n",
    "    def normalize(self, token):\n",
    "        \"\"\"\n",
    "        :param token: string\n",
    "        :return: normalized token type string\n",
    "        \"\"\"\n",
    "        # strip diacritics\n",
    "        token = self.__vocalization.sub('', token)\n",
    "        #strip kasheeda\n",
    "        token = self.__kasheeda.sub('', token)\n",
    "        # strip punctuation marks\n",
    "        token = self.__arabic_punctuation_marks.sub('', token)\n",
    "        # normalize last hamza\n",
    "        for hamza in self.__last_hamzat:\n",
    "            if token.endswith(hamza):\n",
    "                token = suffix_replace(token, hamza, '\\u0621')\n",
    "                break\n",
    "        # normalize other hamzat\n",
    "        token = self.__initial_hamzat.sub('\\u0627', token)\n",
    "        token = self.__waw_hamza.sub('\\u0648', token)\n",
    "        token = self.__yeh_hamza.sub('\\u064a', token)\n",
    "        token = self.__alefat.sub('\\u0627', token)\n",
    "        return token\n",
    "\n",
    "normalizer = ArabicNormalizer()\n",
    "\n",
    "def prepareData(path, sentences):\n",
    "    global normalizer\n",
    "    os.chdir(path);\n",
    "    for ff in glob.glob(\"*\"):\n",
    "        if os.path.isdir(ff):\n",
    "            dPath = path + \"/\" + ff\n",
    "            prepareData(dPath)\n",
    "            continue\n",
    "        fPath = path + \"/\" + ff\n",
    "        count = 0\n",
    "        with open(fPath, 'r', encoding='UTF-8') as f:\n",
    "            for line in f:\n",
    "                if len(line.strip()) == 0:\n",
    "                    continue\n",
    "                count += 1\n",
    "                normalized_words = [normalizer.normalize(s) for s in line.strip().split()]\n",
    "                sentences.append(normalized_words)\n",
    "                if len(sentences)%100 == 0:\n",
    "                    print(\"Load %d lines\"%(len(sentences)), end='\\r')\n",
    "        f.close()\n",
    "        print (\"Got %d lines from file %s\"%(count, fPath))\n",
    "\n",
    "def showTime(ds,de):\n",
    "    result = ''\n",
    "    seconds = (de-ds).total_seconds()\n",
    "    hh = int(seconds/(60*24));\n",
    "    if hh > 0:\n",
    "        result = \"%d h:\"%(hh);\n",
    "    seconds -= hh*60*24\n",
    "    mm = int(seconds/60);\n",
    "    if mm > 0:\n",
    "        result += \"%d min:\"%(mm)\n",
    "    ss = seconds - mm*60;\n",
    "    result += \"%d sec\"%(ss)\n",
    "    return result\n",
    "\n",
    "ds = datetime.datetime.now()                                 \n",
    "prepareData(dataPath, sentences)\n",
    "de = datetime.datetime.now()\n",
    "print (\"At all: got %d lines in %s\"%(len(sentences), showTime(ds,de)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train Word2Vec model\n",
    "Model is created with 100D vectors and trained in 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build vocabulary...\n",
      "Vocabulary is built in 1 min:38 sec\n",
      "Train model...\n",
      "W2V model is completed in 14 h:15 min:1 sec\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "    \n",
    "    def on_epoch_begin(self, model):\n",
    "        print (\"Epoch %d\"%(self.epoch), end='\\r')\n",
    "    \n",
    "    def on_epoch_end(self, model):\n",
    "        self.epoch += 1\n",
    "\n",
    "logger = EpochLogger()        \n",
    "w2v = Word2Vec(size=n_dim, window=10, min_count=3, workers=10)\n",
    "ds = datetime.datetime.now()   \n",
    "print (\"Build vocabulary...\")\n",
    "w2v.build_vocab(sentences)\n",
    "de = datetime.datetime.now()\n",
    "print (\"Vocabulary is built in %s\"%(showTime(ds,de)))\n",
    "print (\"Train model...\")\n",
    "ds = datetime.datetime.now()  \n",
    "w2v.train(sentences, epochs=100, total_examples=len(sentences), callbacks=[logger])\n",
    "de = datetime.datetime.now()\n",
    "print (\"W2V model is completed in %s\"%(showTime(ds,de)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check quality of the Word2Vec model\n",
    "This can be done, for example, by using __most_similar()__ method.    \n",
    "Most (if not all) of the words in its output should have direct connection with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('اموال', 0.7668006420135498),\n",
       " ('ماله', 0.6916441321372986),\n",
       " ('قرض', 0.6779076457023621),\n",
       " ('مالها', 0.6629146337509155),\n",
       " ('ادخار', 0.6276471614837646),\n",
       " ('مدخراته', 0.6235713958740234),\n",
       " ('اموالها', 0.6218032240867615),\n",
       " ('ربح', 0.6210994124412537),\n",
       " ('بالمال', 0.6199471354484558),\n",
       " ('بمال', 0.6176666021347046)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('مال')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Word2Vec model in binary format.\n",
    "Model saved in binary format can be reloaded in the future using **gensim.models.Word2Vec.load()**.    \n",
    "Then it can be used for\n",
    "- word embedding\n",
    "- creating file of vectors\n",
    "- re-train by additional corpora.  \n",
    "\n",
    "_Note: though the gensim interface allows to re-train the existing model, it is not recommended.    \n",
    "The practice would be, when new data arrives, to shuffle it with the \"old\" data and retrain a fresh model with all the data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V model model-2019-Feb-20-223316 is saved in binary format in 4 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelPath = homePath + \"/w2v/models/\"\n",
    "modelName = \"model-%s\"%(datetime.datetime.now().strftime(\"%Y-%b-%d-%H%M%S\"))\n",
    "ds = datetime.datetime.now() \n",
    "w2v.save(modelPath + modelName)\n",
    "de = datetime.datetime.now()\n",
    "print (\"W2V model %s is saved in binary format in %s\\n\"%(modelName, showTime(ds,de)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Word2Vec model in text format (file of vectors)\n",
    "Model saved in text format can be reloaded in the future using **gensim.models.KeyedVectors.load_word2vec_format()**.    \n",
    "Then it can be used for word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2V model model-2019-Feb-20-223316 is saved in the text format in 57 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecPath = homePath + \"/w2v/vectors/\"\n",
    "ds = datetime.datetime.now() \n",
    "w2v.wv.save_word2vec_format(vecPath + modelName + \".vec\", binary=False)\n",
    "de = datetime.datetime.now() \n",
    "print (\"W2V model %s is saved in the text format in %s\\n\"%(modelName, showTime(ds,de)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
