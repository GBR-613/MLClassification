{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Part-Of-Speach tagger \n",
    "A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech (such as noun, verb, adjective, etc.) to each token (so, it implicitly performs tokenization as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install:\n",
    "\n",
    "1.  **Install nltk** (which is the python library containing modules for interfacing with the Stanford taggers):\n",
    "    - Type in Terminal:     \n",
    "        `conda install nltk`    \n",
    "\n",
    "2.  **Get the software**\n",
    "    - Type in Terminal to download the main part of software:  \n",
    "        `wget http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip`\n",
    "    - Unzip downloaded file\n",
    "    - Go to directory stanford-corenlp-full-2016-10-31\n",
    "    - Type in Terminal to download the files, containing Arabic models and properties:        \n",
    "        `wget http://nlp.stanford.edu/software/stanford-arabic-corenlp-2016-10-31-models.jar`    \n",
    "        `wget https://raw.githubusercontent.com/stanfordnlp/CoreNLP/master/src/edu/stanford/nlp/pipeline/StanfordCoreNLP-arabic.properties`\n",
    "\n",
    "3. **Run the local server**\n",
    "    - Type in Terminal:    \n",
    "        `java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-arabic.properties -preload tokenize,ssplit,pos,parse -status_port 9005  -port 9005 -timeout 15000`\n",
    "        \n",
    "Now you can use Stanford POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of using\n",
    "Output of the tagger is an array of tokens with their POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('مشيت', 'VBD'),\n",
       " ('من', 'IN'),\n",
       " ('بيتي', 'NNP'),\n",
       " ('الى', 'IN'),\n",
       " ('المدرسة', 'DTNN'),\n",
       " ('.', 'PUNC')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser(url='http://localhost:9005', tagtype='pos')\n",
    "text = \"مشيتُ من بيتي إلى المدرسة.\"\n",
    "parser.tag(text.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization of the input data\n",
    "Our input data is a collection of documents, containing free Arabic text. Before using them for training and testing our models, we should tokenize their contents and exclude those tokens that do not matter for the classification of documents.\n",
    "So, we can exclude:\n",
    "- Articles (\"DT\")\n",
    "- Punctuation (\"PUNC\")\n",
    "- Conjustions (\"IN\")\n",
    "- Digits (\"CD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
